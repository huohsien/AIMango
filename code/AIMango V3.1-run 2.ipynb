{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = '../C1-P1_Train Dev_fixed/train.csv'\n",
    "VALID_CSV_PATH = '../C1-P1_Train Dev_fixed/dev.csv'\n",
    "TEST_CSV_PATH = '../AIMango_sample/label.csv'\n",
    "\n",
    "ORIGINAL_TRAIN_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "ORIGINAL_VALID_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Dev/' \n",
    "ORIGINAL_TEST_DATA_PATH = '../AIMango_sample/sample_image/' \n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train'\n",
    "VALID_DATA_PATH = '../data/valid'\n",
    "TEST_DATA_PATH = '../data/test'\n",
    "\n",
    "MODEL_WEIGHTS_FILE = 'model_weights_v3_1__run_2.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "new_data = []\n",
    "with open(TEST_CSV_PATH) as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        new_label = row[1][len(row[1])-1]\n",
    "        new_data.append([row[0],new_label])\n",
    "\n",
    "\n",
    "folder, filename = os.path.split(TEST_CSV_PATH)\n",
    "NEW_TEST_CSV_PATH = os.path.join(folder, 'label_new.csv')\n",
    "                                 \n",
    "with open(NEW_TEST_CSV_PATH, 'w') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for row in new_data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "# make file structure for training dataset\n",
    "#\n",
    "with open(TRAIN_CSV_PATH) as csv_file:\n",
    "\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue  #header\n",
    "\n",
    "        src_path = os.path.join(ORIGINAL_TRAIN_DATA_PATH, row[0])\n",
    "        dest_path = os.path.join(TRAIN_DATA_PATH, row[1], row[0])\n",
    "        if not os.path.isfile(dest_path):\n",
    "            copyfile(src_path, dest_path)\n",
    "        \n",
    "        line_count += 1\n",
    "        \n",
    "# make file structure for validation dataset\n",
    "#\n",
    "with open(VALID_CSV_PATH) as csv_file:\n",
    "\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue  #header\n",
    "\n",
    "        src_path = os.path.join(ORIGINAL_VALID_DATA_PATH, row[0])\n",
    "        dest_path = os.path.join(VALID_DATA_PATH, row[1], row[0])\n",
    "        if not os.path.isfile(dest_path):\n",
    "            copyfile(src_path, dest_path)\n",
    "        \n",
    "        line_count += 1\n",
    "\n",
    "# make file structure for validation dataset\n",
    "#\n",
    "with open(NEW_TEST_CSV_PATH) as csv_file:\n",
    "\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue  #header\n",
    "\n",
    "        src_path = os.path.join(ORIGINAL_TEST_DATA_PATH, row[0])\n",
    "        dest_path = os.path.join(TEST_DATA_PATH, row[1], row[0])\n",
    "        if not os.path.isfile(dest_path):\n",
    "            copyfile(src_path, dest_path)\n",
    "        \n",
    "        line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# how many data per batch to load\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "start_epoch = 0\n",
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "valid_loss_stable_count = 10\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(degrees=(-15, 15)),\n",
    "                                transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "transform_plain = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                           ]) \n",
    "\n",
    "loaders_transfer = {}\n",
    "data_transfer = {}\n",
    "\n",
    "data_transfer['train'] = datasets.ImageFolder(TRAIN_DATA_PATH, transform=transform)\n",
    "loaders_transfer['train'] = torch.utils.data.DataLoader(data_transfer['train'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)\n",
    "\n",
    "data_transfer['valid'] = datasets.ImageFolder(VALID_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['valid'] = torch.utils.data.DataLoader(data_transfer['valid'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4)\n",
    "data_transfer['test'] = datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model_transfer.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 3)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = [optim.SGD(model_transfer.fc.parameters(), lr = lr, momentum = 0.9),\n",
    "                      optim.SGD(model_transfer.fc.parameters(), lr = lr * 0.1, momentum = 0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.075557 \tValidation Loss: 1.048149 \tValidation Accuracy: 0.4800 \ttime: 83.7\n",
      "Validation loss decreased from inf to 1.048149. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 1.013513 \tValidation Loss: 0.980093 \tValidation Accuracy: 0.6288 \ttime: 82.1\n",
      "Validation loss decreased from 1.048149 to 0.980093. Model was saved\n",
      "Epoch: 3 \tTraining Loss: 0.940374 \tValidation Loss: 0.905448 \tValidation Accuracy: 0.6625 \ttime: 83.3\n",
      "Validation loss decreased from 0.980093 to 0.905448. Model was saved\n",
      "Epoch: 4 \tTraining Loss: 0.866189 \tValidation Loss: 0.832901 \tValidation Accuracy: 0.6775 \ttime: 82.0\n",
      "Validation loss decreased from 0.905448 to 0.832901. Model was saved\n",
      "Epoch: 5 \tTraining Loss: 0.804241 \tValidation Loss: 0.780893 \tValidation Accuracy: 0.6813 \ttime: 82.6\n",
      "Validation loss decreased from 0.832901 to 0.780893. Model was saved\n",
      "Epoch: 6 \tTraining Loss: 0.754527 \tValidation Loss: 0.740118 \tValidation Accuracy: 0.6863 \ttime: 82.0\n",
      "Validation loss decreased from 0.780893 to 0.740118. Model was saved\n",
      "Epoch: 7 \tTraining Loss: 0.720456 \tValidation Loss: 0.718075 \tValidation Accuracy: 0.6975 \ttime: 83.2\n",
      "Validation loss decreased from 0.740118 to 0.718075. Model was saved\n",
      "Epoch: 8 \tTraining Loss: 0.699487 \tValidation Loss: 0.702938 \tValidation Accuracy: 0.6975 \ttime: 82.9\n",
      "Validation loss decreased from 0.718075 to 0.702938. Model was saved\n",
      "Epoch: 9 \tTraining Loss: 0.686718 \tValidation Loss: 0.698533 \tValidation Accuracy: 0.6987 \ttime: 82.6\n",
      "Validation loss decreased from 0.702938 to 0.698533. Model was saved\n",
      "Epoch: 10 \tTraining Loss: 0.680200 \tValidation Loss: 0.679357 \tValidation Accuracy: 0.7087 \ttime: 83.9\n",
      "Validation loss decreased from 0.698533 to 0.679357. Model was saved\n",
      "Epoch: 11 \tTraining Loss: 0.667362 \tValidation Loss: 0.660142 \tValidation Accuracy: 0.7075 \ttime: 82.6\n",
      "Validation loss decreased from 0.679357 to 0.660142. Model was saved\n",
      "Epoch: 12 \tTraining Loss: 0.656165 \tValidation Loss: 0.661784 \tValidation Accuracy: 0.7025 \ttime: 83.4\n",
      "Epoch: 13 \tTraining Loss: 0.653992 \tValidation Loss: 0.660177 \tValidation Accuracy: 0.7075 \ttime: 83.9\n",
      "Epoch: 14 \tTraining Loss: 0.655408 \tValidation Loss: 0.657661 \tValidation Accuracy: 0.7013 \ttime: 82.5\n",
      "Validation loss decreased from 0.660142 to 0.657661. Model was saved\n",
      "Epoch: 15 \tTraining Loss: 0.649446 \tValidation Loss: 0.656303 \tValidation Accuracy: 0.7125 \ttime: 82.3\n",
      "Validation loss decreased from 0.657661 to 0.656303. Model was saved\n",
      "Epoch: 16 \tTraining Loss: 0.650298 \tValidation Loss: 0.656099 \tValidation Accuracy: 0.7100 \ttime: 82.9\n",
      "Validation loss decreased from 0.656303 to 0.656099. Model was saved\n",
      "Epoch: 17 \tTraining Loss: 0.656575 \tValidation Loss: 0.657932 \tValidation Accuracy: 0.7037 \ttime: 82.5\n",
      "Epoch: 18 \tTraining Loss: 0.650387 \tValidation Loss: 0.654748 \tValidation Accuracy: 0.6987 \ttime: 82.6\n",
      "Validation loss decreased from 0.656099 to 0.654748. Model was saved\n",
      "Epoch: 19 \tTraining Loss: 0.654113 \tValidation Loss: 0.656349 \tValidation Accuracy: 0.7025 \ttime: 82.9\n",
      "Epoch: 20 \tTraining Loss: 0.649688 \tValidation Loss: 0.654881 \tValidation Accuracy: 0.7037 \ttime: 83.9\n",
      "Epoch: 21 \tTraining Loss: 0.648395 \tValidation Loss: 0.651762 \tValidation Accuracy: 0.7175 \ttime: 82.6\n",
      "Validation loss decreased from 0.654748 to 0.651762. Model was saved\n",
      "Epoch: 22 \tTraining Loss: 0.652736 \tValidation Loss: 0.655303 \tValidation Accuracy: 0.7000 \ttime: 83.7\n",
      "Epoch: 23 \tTraining Loss: 0.648358 \tValidation Loss: 0.653342 \tValidation Accuracy: 0.7037 \ttime: 83.1\n",
      "Epoch: 24 \tTraining Loss: 0.648066 \tValidation Loss: 0.653610 \tValidation Accuracy: 0.7050 \ttime: 82.8\n",
      "Epoch: 25 \tTraining Loss: 0.648173 \tValidation Loss: 0.652363 \tValidation Accuracy: 0.7050 \ttime: 81.8\n",
      "Epoch: 26 \tTraining Loss: 0.644865 \tValidation Loss: 0.650703 \tValidation Accuracy: 0.7100 \ttime: 82.3\n",
      "Validation loss decreased from 0.651762 to 0.650703. Model was saved\n",
      "Epoch: 27 \tTraining Loss: 0.648172 \tValidation Loss: 0.649629 \tValidation Accuracy: 0.7188 \ttime: 81.7\n",
      "Validation loss decreased from 0.650703 to 0.649629. Model was saved\n",
      "Epoch: 28 \tTraining Loss: 0.642498 \tValidation Loss: 0.652787 \tValidation Accuracy: 0.7000 \ttime: 84.2\n",
      "Epoch: 29 \tTraining Loss: 0.646066 \tValidation Loss: 0.654977 \tValidation Accuracy: 0.7087 \ttime: 84.1\n",
      "Epoch: 30 \tTraining Loss: 0.642460 \tValidation Loss: 0.651658 \tValidation Accuracy: 0.7100 \ttime: 84.5\n",
      "Epoch: 31 \tTraining Loss: 0.648517 \tValidation Loss: 0.651270 \tValidation Accuracy: 0.7063 \ttime: 83.3\n",
      "Epoch: 32 \tTraining Loss: 0.643831 \tValidation Loss: 0.649160 \tValidation Accuracy: 0.7175 \ttime: 84.2\n",
      "Validation loss decreased from 0.649629 to 0.649160. Model was saved\n",
      "Epoch: 33 \tTraining Loss: 0.642629 \tValidation Loss: 0.648636 \tValidation Accuracy: 0.7163 \ttime: 83.0\n",
      "Validation loss decreased from 0.649160 to 0.648636. Model was saved\n",
      "Epoch: 34 \tTraining Loss: 0.640927 \tValidation Loss: 0.648598 \tValidation Accuracy: 0.7113 \ttime: 81.8\n",
      "Validation loss decreased from 0.648636 to 0.648598. Model was saved\n",
      "Epoch: 35 \tTraining Loss: 0.645294 \tValidation Loss: 0.648031 \tValidation Accuracy: 0.7137 \ttime: 81.9\n",
      "Validation loss decreased from 0.648598 to 0.648031. Model was saved\n",
      "Epoch: 36 \tTraining Loss: 0.636459 \tValidation Loss: 0.649698 \tValidation Accuracy: 0.7113 \ttime: 82.9\n",
      "Epoch: 37 \tTraining Loss: 0.637316 \tValidation Loss: 0.651548 \tValidation Accuracy: 0.7050 \ttime: 84.3\n",
      "Epoch: 38 \tTraining Loss: 0.647183 \tValidation Loss: 0.647753 \tValidation Accuracy: 0.7063 \ttime: 85.1\n",
      "Validation loss decreased from 0.648031 to 0.647753. Model was saved\n",
      "Epoch: 39 \tTraining Loss: 0.646472 \tValidation Loss: 0.647746 \tValidation Accuracy: 0.7125 \ttime: 83.0\n",
      "Validation loss decreased from 0.647753 to 0.647746. Model was saved\n",
      "Epoch: 40 \tTraining Loss: 0.640551 \tValidation Loss: 0.646632 \tValidation Accuracy: 0.7050 \ttime: 82.1\n",
      "Validation loss decreased from 0.647746 to 0.646632. Model was saved\n",
      "Epoch: 41 \tTraining Loss: 0.639738 \tValidation Loss: 0.647234 \tValidation Accuracy: 0.7113 \ttime: 83.5\n",
      "Epoch: 42 \tTraining Loss: 0.638276 \tValidation Loss: 0.648003 \tValidation Accuracy: 0.7075 \ttime: 83.0\n",
      "Epoch: 43 \tTraining Loss: 0.645674 \tValidation Loss: 0.643652 \tValidation Accuracy: 0.7163 \ttime: 83.9\n",
      "Validation loss decreased from 0.646632 to 0.643652. Model was saved\n",
      "Epoch: 44 \tTraining Loss: 0.644825 \tValidation Loss: 0.646341 \tValidation Accuracy: 0.7100 \ttime: 84.5\n",
      "Epoch: 45 \tTraining Loss: 0.639586 \tValidation Loss: 0.644041 \tValidation Accuracy: 0.7200 \ttime: 83.8\n",
      "Epoch: 46 \tTraining Loss: 0.634212 \tValidation Loss: 0.645105 \tValidation Accuracy: 0.7100 \ttime: 83.0\n",
      "Epoch: 47 \tTraining Loss: 0.641931 \tValidation Loss: 0.646757 \tValidation Accuracy: 0.7113 \ttime: 82.4\n",
      "Epoch: 48 \tTraining Loss: 0.637332 \tValidation Loss: 0.644293 \tValidation Accuracy: 0.7175 \ttime: 82.1\n",
      "Epoch: 49 \tTraining Loss: 0.632983 \tValidation Loss: 0.643222 \tValidation Accuracy: 0.7150 \ttime: 82.6\n",
      "Validation loss decreased from 0.643652 to 0.643222. Model was saved\n",
      "Epoch: 50 \tTraining Loss: 0.636691 \tValidation Loss: 0.640289 \tValidation Accuracy: 0.7213 \ttime: 81.1\n",
      "Validation loss decreased from 0.643222 to 0.640289. Model was saved\n",
      "Epoch: 51 \tTraining Loss: 0.639982 \tValidation Loss: 0.646052 \tValidation Accuracy: 0.7100 \ttime: 82.7\n",
      "Epoch: 52 \tTraining Loss: 0.630650 \tValidation Loss: 0.641047 \tValidation Accuracy: 0.7113 \ttime: 81.5\n",
      "Epoch: 53 \tTraining Loss: 0.643022 \tValidation Loss: 0.643740 \tValidation Accuracy: 0.7087 \ttime: 81.1\n",
      "Epoch: 54 \tTraining Loss: 0.633993 \tValidation Loss: 0.643211 \tValidation Accuracy: 0.7150 \ttime: 83.3\n",
      "Epoch: 55 \tTraining Loss: 0.635095 \tValidation Loss: 0.642448 \tValidation Accuracy: 0.7188 \ttime: 84.1\n",
      "Epoch: 56 \tTraining Loss: 0.640301 \tValidation Loss: 0.639821 \tValidation Accuracy: 0.7200 \ttime: 83.9\n",
      "Validation loss decreased from 0.640289 to 0.639821. Model was saved\n",
      "Epoch: 57 \tTraining Loss: 0.633378 \tValidation Loss: 0.643194 \tValidation Accuracy: 0.7075 \ttime: 82.9\n",
      "Epoch: 58 \tTraining Loss: 0.640426 \tValidation Loss: 0.644966 \tValidation Accuracy: 0.7137 \ttime: 82.9\n",
      "Epoch: 59 \tTraining Loss: 0.630507 \tValidation Loss: 0.640566 \tValidation Accuracy: 0.7175 \ttime: 82.5\n",
      "Epoch: 60 \tTraining Loss: 0.641362 \tValidation Loss: 0.639111 \tValidation Accuracy: 0.7150 \ttime: 82.2\n",
      "Validation loss decreased from 0.639821 to 0.639111. Model was saved\n",
      "Epoch: 61 \tTraining Loss: 0.629151 \tValidation Loss: 0.641147 \tValidation Accuracy: 0.7175 \ttime: 80.4\n",
      "Epoch: 62 \tTraining Loss: 0.636368 \tValidation Loss: 0.642259 \tValidation Accuracy: 0.7113 \ttime: 82.8\n",
      "Epoch: 63 \tTraining Loss: 0.640202 \tValidation Loss: 0.641847 \tValidation Accuracy: 0.7200 \ttime: 83.2\n",
      "Epoch: 64 \tTraining Loss: 0.634791 \tValidation Loss: 0.640244 \tValidation Accuracy: 0.7175 \ttime: 83.8\n",
      "Epoch: 65 \tTraining Loss: 0.634236 \tValidation Loss: 0.640658 \tValidation Accuracy: 0.7125 \ttime: 81.7\n",
      "Epoch: 66 \tTraining Loss: 0.634077 \tValidation Loss: 0.639501 \tValidation Accuracy: 0.7188 \ttime: 82.5\n",
      "Epoch: 67 \tTraining Loss: 0.642259 \tValidation Loss: 0.642609 \tValidation Accuracy: 0.7137 \ttime: 82.6\n",
      "Epoch: 68 \tTraining Loss: 0.637129 \tValidation Loss: 0.638233 \tValidation Accuracy: 0.7200 \ttime: 81.3\n",
      "Validation loss decreased from 0.639111 to 0.638233. Model was saved\n",
      "Epoch: 69 \tTraining Loss: 0.630485 \tValidation Loss: 0.636204 \tValidation Accuracy: 0.7213 \ttime: 82.5\n",
      "Validation loss decreased from 0.638233 to 0.636204. Model was saved\n",
      "Epoch: 70 \tTraining Loss: 0.641940 \tValidation Loss: 0.637469 \tValidation Accuracy: 0.7213 \ttime: 81.9\n",
      "Epoch: 71 \tTraining Loss: 0.632095 \tValidation Loss: 0.636049 \tValidation Accuracy: 0.7200 \ttime: 82.9\n",
      "Validation loss decreased from 0.636204 to 0.636049. Model was saved\n",
      "Epoch: 72 \tTraining Loss: 0.632293 \tValidation Loss: 0.640448 \tValidation Accuracy: 0.7150 \ttime: 79.6\n",
      "Epoch: 73 \tTraining Loss: 0.634356 \tValidation Loss: 0.638719 \tValidation Accuracy: 0.7225 \ttime: 81.2\n",
      "Epoch: 74 \tTraining Loss: 0.636558 \tValidation Loss: 0.638678 \tValidation Accuracy: 0.7175 \ttime: 83.2\n",
      "Epoch: 75 \tTraining Loss: 0.635098 \tValidation Loss: 0.638130 \tValidation Accuracy: 0.7175 \ttime: 81.7\n",
      "Epoch: 76 \tTraining Loss: 0.629101 \tValidation Loss: 0.639428 \tValidation Accuracy: 0.7213 \ttime: 82.2\n",
      "Epoch: 77 \tTraining Loss: 0.629307 \tValidation Loss: 0.638138 \tValidation Accuracy: 0.7175 \ttime: 83.4\n",
      "Epoch: 78 \tTraining Loss: 0.633469 \tValidation Loss: 0.637620 \tValidation Accuracy: 0.7200 \ttime: 82.2\n",
      "Epoch: 79 \tTraining Loss: 0.635211 \tValidation Loss: 0.635761 \tValidation Accuracy: 0.7175 \ttime: 82.4\n",
      "Validation loss decreased from 0.636049 to 0.635761. Model was saved\n",
      "Epoch: 80 \tTraining Loss: 0.636924 \tValidation Loss: 0.636702 \tValidation Accuracy: 0.7200 \ttime: 82.2\n",
      "Epoch: 81 \tTraining Loss: 0.633439 \tValidation Loss: 0.636139 \tValidation Accuracy: 0.7175 \ttime: 81.4\n",
      "Epoch: 82 \tTraining Loss: 0.625235 \tValidation Loss: 0.634485 \tValidation Accuracy: 0.7213 \ttime: 81.5\n",
      "Validation loss decreased from 0.635761 to 0.634485. Model was saved\n",
      "Epoch: 83 \tTraining Loss: 0.628011 \tValidation Loss: 0.636303 \tValidation Accuracy: 0.7225 \ttime: 81.3\n",
      "Epoch: 84 \tTraining Loss: 0.626284 \tValidation Loss: 0.634237 \tValidation Accuracy: 0.7250 \ttime: 81.4\n",
      "Validation loss decreased from 0.634485 to 0.634237. Model was saved\n",
      "Epoch: 85 \tTraining Loss: 0.632440 \tValidation Loss: 0.635749 \tValidation Accuracy: 0.7213 \ttime: 82.7\n",
      "Epoch: 86 \tTraining Loss: 0.625061 \tValidation Loss: 0.635006 \tValidation Accuracy: 0.7150 \ttime: 81.1\n",
      "Epoch: 87 \tTraining Loss: 0.630744 \tValidation Loss: 0.634710 \tValidation Accuracy: 0.7238 \ttime: 83.9\n",
      "Epoch: 88 \tTraining Loss: 0.631799 \tValidation Loss: 0.633231 \tValidation Accuracy: 0.7213 \ttime: 81.4\n",
      "Validation loss decreased from 0.634237 to 0.633231. Model was saved\n",
      "Epoch: 89 \tTraining Loss: 0.630382 \tValidation Loss: 0.637277 \tValidation Accuracy: 0.7175 \ttime: 83.2\n",
      "Epoch: 90 \tTraining Loss: 0.628807 \tValidation Loss: 0.635148 \tValidation Accuracy: 0.7238 \ttime: 83.0\n",
      "Epoch: 91 \tTraining Loss: 0.629525 \tValidation Loss: 0.635816 \tValidation Accuracy: 0.7213 \ttime: 82.5\n",
      "Epoch: 92 \tTraining Loss: 0.629426 \tValidation Loss: 0.632817 \tValidation Accuracy: 0.7238 \ttime: 82.5\n",
      "Validation loss decreased from 0.633231 to 0.632817. Model was saved\n",
      "Epoch: 93 \tTraining Loss: 0.627516 \tValidation Loss: 0.633264 \tValidation Accuracy: 0.7250 \ttime: 81.8\n",
      "Epoch: 94 \tTraining Loss: 0.626698 \tValidation Loss: 0.634540 \tValidation Accuracy: 0.7200 \ttime: 82.1\n",
      "Epoch: 95 \tTraining Loss: 0.626465 \tValidation Loss: 0.631201 \tValidation Accuracy: 0.7250 \ttime: 82.2\n",
      "Validation loss decreased from 0.632817 to 0.631201. Model was saved\n",
      "Epoch: 96 \tTraining Loss: 0.625562 \tValidation Loss: 0.633158 \tValidation Accuracy: 0.7188 \ttime: 84.3\n",
      "Epoch: 97 \tTraining Loss: 0.631942 \tValidation Loss: 0.633006 \tValidation Accuracy: 0.7213 \ttime: 82.6\n",
      "Epoch: 98 \tTraining Loss: 0.623403 \tValidation Loss: 0.630510 \tValidation Accuracy: 0.7213 \ttime: 81.8\n",
      "Validation loss decreased from 0.631201 to 0.630510. Model was saved\n",
      "Epoch: 99 \tTraining Loss: 0.630462 \tValidation Loss: 0.632791 \tValidation Accuracy: 0.7238 \ttime: 81.2\n",
      "Epoch: 100 \tTraining Loss: 0.627670 \tValidation Loss: 0.632820 \tValidation Accuracy: 0.7188 \ttime: 81.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train(n_epochs, loaders, model, optimizers, criterion, save_path):\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    # Valid Loss Stable counter\n",
    "    valid_loss_stable_counter = 0\n",
    "    #optimizer index\n",
    "    optim_idx = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        optimizer = optimizers[optim_idx]\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # train the model \n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # validate the model \n",
    "        \n",
    "        model.eval()\n",
    "        valid_corrects = 0\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['valid']):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "            valid_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "        train_loss = train_loss/len(loaders_transfer['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders_transfer['valid'].dataset)\n",
    "        \n",
    "        epoch_acc = valid_corrects.double() / len(loaders_transfer['valid'].dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.4f} \\ttime: {:.1f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            epoch_acc,\n",
    "            time.time() - start\n",
    "            ))\n",
    "        \n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, save_path)\n",
    "\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "            valid_loss_stable_counter = 0\n",
    "        else:\n",
    "            valid_loss_stable_counter += 1\n",
    "            optim_idx = 1\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "model_transfer = train(100, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, MODEL_WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# # list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "# class_names = data_transfer['train'].classes\n",
    "\n",
    "# def predict_class_transfer(img_path):\n",
    "#     # load the image and return the predicted breed\n",
    "#     img = Image.open(img_path)\n",
    "\n",
    "#     transform = transforms.Compose([\n",
    "#                                 transforms.Resize(224),\n",
    "#                                 transforms.CenterCrop(224),\n",
    "#                                 transforms.ToTensor()\n",
    "#                                ]) \n",
    "#     img = transform(img)\n",
    "#     img = img.unsqueeze(0) \n",
    "\n",
    "#     img = Variable(img)\n",
    "\n",
    "#     img = img.to(device)\n",
    "        \n",
    "#     prediction = model_transfer(img)  # Returns a Tensor of shape (batch, num class labels)\n",
    "# #     print(prediction)\n",
    "# #     print(class_names)\n",
    "# #     prediction = prediction.data.max(1, keepdim=True)[1]\n",
    "#     prediction = prediction.data.cpu().numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n",
    "#     prediction = class_names[prediction]\n",
    "#     return prediction \n",
    "\n",
    "\n",
    "# # predict_class_transfer('../data/competition/02186.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get all test files\n",
    "\n",
    "# from glob import glob\n",
    "# import numpy as np\n",
    "\n",
    "# test_results = []\n",
    "\n",
    "# mango_files = np.array(glob(\"../data/competition/*\"))\n",
    "\n",
    "# for idx, file in enumerate(mango_files):\n",
    "#     _ , filename = os.path.split(file)\n",
    "#     className = predict_class_transfer(file)\n",
    "#     test_results.append([filename, className])\n",
    "    \n",
    "# # test_results[:3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# with open('results.csv', 'w') as f:\n",
    "\n",
    "#     writer = csv.writer(f)\n",
    "    \n",
    "#     for row in test_results:\n",
    "#         writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.559737\n",
      "\n",
      "\n",
      "Test Accuracy: 69% (134/193)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_transfer, model_transfer, criterion_transfer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
