{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = '../C1-P1_Train Dev_fixed/train.csv'\n",
    "VALID_CSV_PATH = '../C1-P1_Train Dev_fixed/dev.csv'\n",
    "TEST_CSV_PATH = '../AIMango_sample/label.csv'\n",
    "\n",
    "ORIGINAL_TRAIN_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "ORIGINAL_VALID_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Dev/' \n",
    "ORIGINAL_TEST_DATA_PATH = '../AIMango_sample/sample_image/' \n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train'\n",
    "VALID_DATA_PATH = '../data/valid'\n",
    "TEST_DATA_PATH = '../data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "new_data = []\n",
    "with open(TEST_CSV_PATH) as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        new_label = row[1][len(row[1])-1]\n",
    "        new_data.append([row[0],new_label])\n",
    "\n",
    "\n",
    "folder, filename = os.path.split(TEST_CSV_PATH)\n",
    "NEW_TEST_CSV_PATH = os.path.join(folder, 'label_new.csv')\n",
    "                                 \n",
    "with open(NEW_TEST_CSV_PATH, 'w') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for row in new_data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "# make file structure for training dataset\n",
    "#\n",
    "with open(TRAIN_CSV_PATH) as csv_file:\n",
    "\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue  #header\n",
    "\n",
    "        src_path = os.path.join(ORIGINAL_TRAIN_DATA_PATH, row[0])\n",
    "        dest_path = os.path.join(TRAIN_DATA_PATH, row[1], row[0])\n",
    "        if not os.path.isfile(dest_path):\n",
    "            copyfile(src_path, dest_path)\n",
    "        \n",
    "        line_count += 1\n",
    "        \n",
    "# make file structure for validation dataset\n",
    "#\n",
    "with open(VALID_CSV_PATH) as csv_file:\n",
    "\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue  #header\n",
    "\n",
    "        src_path = os.path.join(ORIGINAL_VALID_DATA_PATH, row[0])\n",
    "        dest_path = os.path.join(VALID_DATA_PATH, row[1], row[0])\n",
    "        if not os.path.isfile(dest_path):\n",
    "            copyfile(src_path, dest_path)\n",
    "        \n",
    "        line_count += 1\n",
    "\n",
    "# make file structure for validation dataset\n",
    "#\n",
    "with open(NEW_TEST_CSV_PATH) as csv_file:\n",
    "\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue  #header\n",
    "\n",
    "        src_path = os.path.join(ORIGINAL_TEST_DATA_PATH, row[0])\n",
    "        dest_path = os.path.join(TEST_DATA_PATH, row[1], row[0])\n",
    "        if not os.path.isfile(dest_path):\n",
    "            copyfile(src_path, dest_path)\n",
    "        \n",
    "        line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# how many data per batch to load\n",
    "batch_size = 32\n",
    "start_epoch = 0\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(degrees=(-15, 15)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "loaders_transfer = {}\n",
    "data_transfer = {}\n",
    "\n",
    "data_transfer['train'] = datasets.ImageFolder(TRAIN_DATA_PATH, transform=transform)\n",
    "loaders_transfer['train'] = torch.utils.data.DataLoader(data_transfer['train'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "data_transfer['valid'] = datasets.ImageFolder(VALID_DATA_PATH, transform=transform)\n",
    "loaders_transfer['valid'] = torch.utils.data.DataLoader(data_transfer['valid'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "data_transfer['test'] = datasets.ImageFolder(TEST_DATA_PATH, transform=transform)\n",
    "loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "model_transfer = models.resnet50(pretrained=True).to(device)\n",
    "    \n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model_transfer.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 3)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.models as models\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # check if CUDA is available\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# ## TODO: Specify model architecture \n",
    "# model_transfer = models.vgg16(pretrained=True)\n",
    "\n",
    "# for param in model_transfer.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# from collections import OrderedDict\n",
    "# classifier = nn.Sequential(OrderedDict([\n",
    "#                           ('fc1', nn.Linear(25088, 1024)),\n",
    "#                           ('relu', nn.ReLU()),\n",
    "#                           ('dropout', nn.Dropout(0.25)),\n",
    "#                           ('fc2', nn.Linear(1024, 3)),\n",
    "#                           ('logsoftmax', nn.LogSoftmax(dim=1))\n",
    "#                           ]))\n",
    "    \n",
    "# model_transfer.classifier = classifier\n",
    "\n",
    "\n",
    "\n",
    "# if use_cuda:\n",
    "#     model_transfer = model_transfer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# criterion_transfer = nn.NLLLoss()\n",
    "# optimizer_transfer = optim.SGD(model_transfer.classifier.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.Adam(model_transfer.fc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.840045 \tValidation Loss: 0.708563 \t time: 169.3\n",
      "Validation loss decreased from inf to 0.708563. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 0.687470 \tValidation Loss: 0.663397 \t time: 162.2\n",
      "Validation loss decreased from 0.708563 to 0.663397. Model was saved\n",
      "Epoch: 3 \tTraining Loss: 0.646825 \tValidation Loss: 0.618083 \t time: 161.2\n",
      "Validation loss decreased from 0.663397 to 0.618083. Model was saved\n",
      "Epoch: 4 \tTraining Loss: 0.672033 \tValidation Loss: 0.669437 \t time: 161.2\n",
      "Epoch: 5 \tTraining Loss: 0.647924 \tValidation Loss: 0.612102 \t time: 161.5\n",
      "Validation loss decreased from 0.618083 to 0.612102. Model was saved\n",
      "Epoch: 6 \tTraining Loss: 0.651291 \tValidation Loss: 0.773446 \t time: 162.1\n",
      "Epoch: 7 \tTraining Loss: 0.659110 \tValidation Loss: 0.676000 \t time: 162.3\n",
      "Epoch: 8 \tTraining Loss: 0.634901 \tValidation Loss: 0.625091 \t time: 161.2\n",
      "Epoch: 9 \tTraining Loss: 0.633858 \tValidation Loss: 0.598476 \t time: 160.9\n",
      "Validation loss decreased from 0.612102 to 0.598476. Model was saved\n",
      "Epoch: 10 \tTraining Loss: 0.645942 \tValidation Loss: 0.602484 \t time: 161.4\n",
      "Epoch: 11 \tTraining Loss: 0.631658 \tValidation Loss: 0.595701 \t time: 162.3\n",
      "Validation loss decreased from 0.598476 to 0.595701. Model was saved\n",
      "Epoch: 12 \tTraining Loss: 0.626591 \tValidation Loss: 0.652543 \t time: 160.1\n",
      "Epoch: 13 \tTraining Loss: 0.626191 \tValidation Loss: 0.623921 \t time: 162.1\n",
      "Epoch: 14 \tTraining Loss: 0.625496 \tValidation Loss: 0.579367 \t time: 160.3\n",
      "Validation loss decreased from 0.595701 to 0.579367. Model was saved\n",
      "Epoch: 15 \tTraining Loss: 0.612402 \tValidation Loss: 0.601179 \t time: 160.6\n",
      "Epoch: 16 \tTraining Loss: 0.611688 \tValidation Loss: 0.623021 \t time: 162.1\n",
      "Epoch: 17 \tTraining Loss: 0.617934 \tValidation Loss: 0.594860 \t time: 162.8\n",
      "Epoch: 18 \tTraining Loss: 0.608243 \tValidation Loss: 0.611396 \t time: 162.4\n",
      "Epoch: 19 \tTraining Loss: 0.611187 \tValidation Loss: 0.583104 \t time: 160.7\n",
      "Epoch: 20 \tTraining Loss: 0.609509 \tValidation Loss: 0.567167 \t time: 163.0\n",
      "Validation loss decreased from 0.579367 to 0.567167. Model was saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update accumulated training loss\n",
    "#             train_loss += loss.item()*data.size(0)\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['valid']):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update accumulated validation loss \n",
    "#             valid_loss += loss.item()*data.size(0)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "#         train_loss = train_loss/len(loaders_transfer['train'].dataset)\n",
    "#         valid_loss = valid_loss/len(loaders_transfer['valid'].dataset)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t time: {:.1f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            time.time() - start\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "            \n",
    "            torch.save(model.state_dict(), 'model_transfer.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "model_transfer = train(20, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, 'model_transfer.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the model\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# def train(n_epochs, loaders, model, optimizer, criterion, save_path):\n",
    "\n",
    "#     valid_loss_min = np.Inf \n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         print('Epoch {}/{}'.format(epoch+1, n_epochs))\n",
    "#         print('-' * 10)\n",
    "#         start = time.time()\n",
    "\n",
    "#         for phase in ['train', 'valid']:\n",
    "#             if phase == 'train':\n",
    "#                 model.train()\n",
    "#             else:\n",
    "#                 model.eval()\n",
    "\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "\n",
    "#             for inputs, labels in loaders[phase]:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "#                 if phase == 'train':\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss.backward()\n",
    "#                     optimizer.step()\n",
    "\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / len(data_transfer[phase])\n",
    "#             epoch_acc = running_corrects.double() / len(data_transfer[phase])\n",
    "\n",
    "#             print('{} loss: {:.4f}, acc: {:.4f}, time: {:.1f}'.format(phase, epoch_loss, epoch_acc, time.time() - start))\n",
    "    \n",
    "\n",
    "#     return model\n",
    "\n",
    "# model_transfer = train(20, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, 'model_transfer.pt')\n",
    "\n",
    "# # load the model that got the best validation accuracy (uncomment the line below)\n",
    "# model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "#     # monitor test loss and accuracy\n",
    "#     test_loss = 0.\n",
    "#     correct = 0.\n",
    "#     total = 0.\n",
    "\n",
    "#     model.eval()\n",
    "#     for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "#         # move to GPU\n",
    "#         if use_cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "#         # forward pass: compute predicted outputs by passing inputs to the model\n",
    "#         output = model(data)\n",
    "#         # calculate the loss\n",
    "#         loss = criterion(output, target)\n",
    "#         # update average test loss \n",
    "#         test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "#         # convert output probabilities to predicted class\n",
    "#         pred = output.data.max(1, keepdim=True)[1]\n",
    "#         # compare predictions to true label\n",
    "#         correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "#         total += data.size(0)\n",
    "            \n",
    "#     print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "#     print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "#         100. * correct / total, correct, total))\n",
    "\n",
    "# # call test function    \n",
    "# test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = data_transfer['train'].classes\n",
    "\n",
    "def predict_class_transfer(img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()\n",
    "                               ]) \n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0) \n",
    "\n",
    "    img = Variable(img)\n",
    "\n",
    "    img = img.to(device)\n",
    "        \n",
    "    prediction = model_transfer(img)  # Returns a Tensor of shape (batch, num class labels)\n",
    "#     print(prediction)\n",
    "#     print(class_names)\n",
    "#     prediction = prediction.data.max(1, keepdim=True)[1]\n",
    "    prediction = prediction.data.cpu().numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n",
    "    prediction = class_names[prediction]\n",
    "    return prediction \n",
    "\n",
    "\n",
    "predict_class_transfer('../data/competition/02186.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['04108.jpg', 'A'], ['03703.jpg', 'A'], ['06214.jpg', 'A']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all test files\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "test_results = []\n",
    "\n",
    "mango_files = np.array(glob(\"../data/competition/*\"))\n",
    "\n",
    "for idx, file in enumerate(mango_files):\n",
    "    _ , filename = os.path.split(file)\n",
    "    className = predict_class_transfer(file)\n",
    "    test_results.append([filename, className])\n",
    "    \n",
    "test_results[:3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('results.csv', 'w') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for row in test_results:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.746572\n",
      "\n",
      "\n",
      "Test Accuracy: 66% (62/93)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_transfer, model_transfer, criterion_transfer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
