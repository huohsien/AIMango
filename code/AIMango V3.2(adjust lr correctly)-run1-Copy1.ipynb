{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#### batch_size = 128\n",
    "#### num_training_epochs = 400\n",
    "#### lr = 0.01\n",
    "#### valid_loss_stable_count = 10\n",
    "\n",
    "#### Make learning rate lr one tenth if the number of epochs in which validation loss doesn't decrease exceeds the paramter of valid_loss_stable_count.\n",
    "\n",
    "#### Test Loss: 0.678148\n",
    "#### Test Accuracy: 70% (140/200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_CSV_PATH = '../C1-P1_Train Dev_fixed/train.csv'\n",
    "TRAIN_CSV_PATH = '../C1-P1_Train Dev_fixed/train_split.csv'\n",
    "VALID_CSV_PATH = '../C1-P1_Train Dev_fixed/dev.csv'\n",
    "\n",
    "ORIGINAL_TRAIN_TEST_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "ORIGINAL_VALID_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Dev/' \n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train'\n",
    "VALID_DATA_PATH = '../data/valid'\n",
    "TEST_DATA_PATH = '../data/test'\n",
    "\n",
    "# ToChange!!!\n",
    "MODEL_WEIGHTS_FILE = 'model_weights_v3_2_run_1.pt'\n",
    "\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "valid_loss_stable_count = 10  # and when valid loss is stable, adjust lr\n",
    "lr_decay_factor = 0.1\n",
    "lr_lower_bound = 1e-4\n",
    "num_training_epochs = 400\n",
    "\n",
    "num_worker = 6\n",
    "sgd_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(degrees=(-15, 15)),\n",
    "                                transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "transform_plain = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                           ]) \n",
    "\n",
    "loaders_transfer = {}\n",
    "data_transfer = {}\n",
    "\n",
    "data_transfer['train'] = torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, transform=transform)\n",
    "loaders_transfer['train'] = torch.utils.data.DataLoader(data_transfer['train'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_worker)\n",
    "\n",
    "data_transfer['valid'] = torchvision.datasets.ImageFolder(VALID_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['valid'] = torch.utils.data.DataLoader(data_transfer['valid'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_worker)\n",
    "data_transfer['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model_transfer.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 3)).to(device)\n",
    "## uncomment the following line for continuing trainging only\n",
    "# model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.SGD(model_transfer.fc.parameters(), lr = lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_from_optim(optimizer):\n",
    "    for param_group in optimizer_transfer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr_to_optim(optimizer, lr):\n",
    "    for param_group in optimizer_transfer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.9085 \tValidation Loss: 0.7200 \tValidation Accuracy: 0.672 \ttime: 72.2 \tlr=0.01\n",
      "Validation loss decreased from inf to 0.720048. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 0.7089 \tValidation Loss: 0.8144 \tValidation Accuracy: 0.629 \ttime: 74.3 \tlr=0.01\n",
      "Epoch: 3 \tTraining Loss: 0.6856 \tValidation Loss: 0.6759 \tValidation Accuracy: 0.686 \ttime: 74.1 \tlr=0.01\n",
      "Validation loss decreased from 0.720048 to 0.675897. Model was saved\n",
      "Epoch: 4 \tTraining Loss: 0.6617 \tValidation Loss: 0.6612 \tValidation Accuracy: 0.706 \ttime: 74.8 \tlr=0.01\n",
      "Validation loss decreased from 0.675897 to 0.661168. Model was saved\n",
      "Epoch: 5 \tTraining Loss: 0.6383 \tValidation Loss: 0.6228 \tValidation Accuracy: 0.734 \ttime: 73.9 \tlr=0.01\n",
      "Validation loss decreased from 0.661168 to 0.622833. Model was saved\n",
      "Epoch: 6 \tTraining Loss: 0.6465 \tValidation Loss: 0.6665 \tValidation Accuracy: 0.691 \ttime: 73.1 \tlr=0.01\n",
      "Epoch: 7 \tTraining Loss: 0.6412 \tValidation Loss: 0.6420 \tValidation Accuracy: 0.720 \ttime: 74.9 \tlr=0.01\n",
      "Epoch: 8 \tTraining Loss: 0.6317 \tValidation Loss: 0.6279 \tValidation Accuracy: 0.731 \ttime: 74.1 \tlr=0.01\n",
      "Epoch: 9 \tTraining Loss: 0.6105 \tValidation Loss: 0.6186 \tValidation Accuracy: 0.733 \ttime: 74.1 \tlr=0.01\n",
      "Validation loss decreased from 0.622833 to 0.618575. Model was saved\n",
      "Epoch: 10 \tTraining Loss: 0.6305 \tValidation Loss: 0.6180 \tValidation Accuracy: 0.725 \ttime: 72.4 \tlr=0.01\n",
      "Validation loss decreased from 0.618575 to 0.617965. Model was saved\n",
      "Epoch: 11 \tTraining Loss: 0.6344 \tValidation Loss: 0.6268 \tValidation Accuracy: 0.723 \ttime: 72.4 \tlr=0.01\n",
      "Epoch: 12 \tTraining Loss: 0.6308 \tValidation Loss: 0.6365 \tValidation Accuracy: 0.714 \ttime: 74.2 \tlr=0.01\n",
      "Epoch: 13 \tTraining Loss: 0.6339 \tValidation Loss: 0.6515 \tValidation Accuracy: 0.715 \ttime: 72.4 \tlr=0.01\n",
      "Epoch: 14 \tTraining Loss: 0.6210 \tValidation Loss: 0.6238 \tValidation Accuracy: 0.735 \ttime: 70.3 \tlr=0.01\n",
      "Epoch: 15 \tTraining Loss: 0.5910 \tValidation Loss: 0.6253 \tValidation Accuracy: 0.723 \ttime: 73.7 \tlr=0.01\n",
      "Epoch: 16 \tTraining Loss: 0.6131 \tValidation Loss: 0.6991 \tValidation Accuracy: 0.703 \ttime: 73.5 \tlr=0.01\n",
      "Epoch: 17 \tTraining Loss: 0.6329 \tValidation Loss: 0.6056 \tValidation Accuracy: 0.734 \ttime: 72.3 \tlr=0.01\n",
      "Validation loss decreased from 0.617965 to 0.605644. Model was saved\n",
      "Epoch: 18 \tTraining Loss: 0.6190 \tValidation Loss: 0.6054 \tValidation Accuracy: 0.734 \ttime: 73.7 \tlr=0.01\n",
      "Validation loss decreased from 0.605644 to 0.605378. Model was saved\n",
      "Epoch: 19 \tTraining Loss: 0.5939 \tValidation Loss: 0.6208 \tValidation Accuracy: 0.715 \ttime: 73.7 \tlr=0.01\n",
      "Epoch: 20 \tTraining Loss: 0.6018 \tValidation Loss: 0.5985 \tValidation Accuracy: 0.730 \ttime: 73.2 \tlr=0.01\n",
      "Validation loss decreased from 0.605378 to 0.598529. Model was saved\n",
      "Epoch: 21 \tTraining Loss: 0.6022 \tValidation Loss: 0.6774 \tValidation Accuracy: 0.699 \ttime: 74.3 \tlr=0.01\n",
      "Epoch: 22 \tTraining Loss: 0.5913 \tValidation Loss: 0.6139 \tValidation Accuracy: 0.738 \ttime: 74.5 \tlr=0.01\n",
      "Epoch: 23 \tTraining Loss: 0.6012 \tValidation Loss: 0.6798 \tValidation Accuracy: 0.685 \ttime: 73.1 \tlr=0.01\n",
      "Epoch: 24 \tTraining Loss: 0.5783 \tValidation Loss: 0.5885 \tValidation Accuracy: 0.739 \ttime: 73.4 \tlr=0.01\n",
      "Validation loss decreased from 0.598529 to 0.588465. Model was saved\n",
      "Epoch: 25 \tTraining Loss: 0.5841 \tValidation Loss: 0.6066 \tValidation Accuracy: 0.721 \ttime: 73.2 \tlr=0.01\n",
      "Epoch: 26 \tTraining Loss: 0.5776 \tValidation Loss: 0.5971 \tValidation Accuracy: 0.734 \ttime: 74.2 \tlr=0.01\n",
      "Epoch: 27 \tTraining Loss: 0.5641 \tValidation Loss: 0.6371 \tValidation Accuracy: 0.716 \ttime: 73.1 \tlr=0.01\n",
      "Epoch: 28 \tTraining Loss: 0.5752 \tValidation Loss: 0.5924 \tValidation Accuracy: 0.735 \ttime: 73.7 \tlr=0.01\n",
      "Epoch: 29 \tTraining Loss: 0.5679 \tValidation Loss: 0.6042 \tValidation Accuracy: 0.741 \ttime: 73.2 \tlr=0.01\n",
      "Epoch: 30 \tTraining Loss: 0.5606 \tValidation Loss: 0.6027 \tValidation Accuracy: 0.721 \ttime: 72.7 \tlr=0.01\n",
      "Epoch: 31 \tTraining Loss: 0.5753 \tValidation Loss: 0.6160 \tValidation Accuracy: 0.730 \ttime: 72.7 \tlr=0.01\n",
      "Epoch: 32 \tTraining Loss: 0.5688 \tValidation Loss: 0.5891 \tValidation Accuracy: 0.739 \ttime: 72.5 \tlr=0.01\n",
      "Epoch: 33 \tTraining Loss: 0.5586 \tValidation Loss: 0.5929 \tValidation Accuracy: 0.733 \ttime: 72.9 \tlr=0.01\n",
      "Epoch: 34 \tTraining Loss: 0.5513 \tValidation Loss: 0.5918 \tValidation Accuracy: 0.735 \ttime: 74.2 \tlr=0.01\n",
      "Epoch: 35 \tTraining Loss: 0.5303 \tValidation Loss: 0.5819 \tValidation Accuracy: 0.750 \ttime: 72.0 \tlr=0.001\n",
      "Validation loss decreased from 0.588465 to 0.581906. Model was saved\n",
      "Epoch: 36 \tTraining Loss: 0.5343 \tValidation Loss: 0.5935 \tValidation Accuracy: 0.749 \ttime: 73.2 \tlr=0.001\n",
      "Epoch: 37 \tTraining Loss: 0.5292 \tValidation Loss: 0.5792 \tValidation Accuracy: 0.741 \ttime: 73.9 \tlr=0.001\n",
      "Validation loss decreased from 0.581906 to 0.579161. Model was saved\n",
      "Epoch: 38 \tTraining Loss: 0.5262 \tValidation Loss: 0.5870 \tValidation Accuracy: 0.755 \ttime: 73.9 \tlr=0.001\n",
      "Epoch: 39 \tTraining Loss: 0.5246 \tValidation Loss: 0.6003 \tValidation Accuracy: 0.739 \ttime: 74.0 \tlr=0.001\n",
      "Epoch: 40 \tTraining Loss: 0.5274 \tValidation Loss: 0.5847 \tValidation Accuracy: 0.746 \ttime: 74.2 \tlr=0.001\n",
      "Epoch: 41 \tTraining Loss: 0.5228 \tValidation Loss: 0.5833 \tValidation Accuracy: 0.745 \ttime: 73.1 \tlr=0.001\n",
      "Epoch: 42 \tTraining Loss: 0.5234 \tValidation Loss: 0.5836 \tValidation Accuracy: 0.753 \ttime: 72.9 \tlr=0.001\n",
      "Epoch: 43 \tTraining Loss: 0.5185 \tValidation Loss: 0.5916 \tValidation Accuracy: 0.753 \ttime: 73.4 \tlr=0.001\n",
      "Epoch: 44 \tTraining Loss: 0.5161 \tValidation Loss: 0.5855 \tValidation Accuracy: 0.729 \ttime: 74.1 \tlr=0.001\n",
      "Epoch: 45 \tTraining Loss: 0.5217 \tValidation Loss: 0.5864 \tValidation Accuracy: 0.754 \ttime: 73.6 \tlr=0.001\n",
      "Epoch: 46 \tTraining Loss: 0.5317 \tValidation Loss: 0.5821 \tValidation Accuracy: 0.744 \ttime: 74.0 \tlr=0.001\n",
      "Epoch: 47 \tTraining Loss: 0.5202 \tValidation Loss: 0.5814 \tValidation Accuracy: 0.749 \ttime: 73.6 \tlr=0.001\n",
      "Epoch: 48 \tTraining Loss: 0.5197 \tValidation Loss: 0.5839 \tValidation Accuracy: 0.745 \ttime: 73.9 \tlr=0.0001\n",
      "Epoch: 49 \tTraining Loss: 0.5198 \tValidation Loss: 0.5822 \tValidation Accuracy: 0.748 \ttime: 75.4 \tlr=0.0001\n",
      "Epoch: 50 \tTraining Loss: 0.5278 \tValidation Loss: 0.5837 \tValidation Accuracy: 0.751 \ttime: 74.5 \tlr=0.0001\n",
      "Epoch: 51 \tTraining Loss: 0.5164 \tValidation Loss: 0.5846 \tValidation Accuracy: 0.753 \ttime: 74.2 \tlr=0.0001\n",
      "Epoch: 52 \tTraining Loss: 0.5186 \tValidation Loss: 0.5811 \tValidation Accuracy: 0.745 \ttime: 72.3 \tlr=0.0001\n",
      "Epoch: 53 \tTraining Loss: 0.5236 \tValidation Loss: 0.5813 \tValidation Accuracy: 0.746 \ttime: 73.1 \tlr=0.0001\n",
      "Epoch: 54 \tTraining Loss: 0.5261 \tValidation Loss: 0.5798 \tValidation Accuracy: 0.740 \ttime: 75.1 \tlr=0.0001\n",
      "Epoch: 55 \tTraining Loss: 0.5182 \tValidation Loss: 0.5812 \tValidation Accuracy: 0.755 \ttime: 75.5 \tlr=0.0001\n",
      "Epoch: 56 \tTraining Loss: 0.5185 \tValidation Loss: 0.5800 \tValidation Accuracy: 0.749 \ttime: 72.7 \tlr=0.0001\n",
      "Epoch: 57 \tTraining Loss: 0.5170 \tValidation Loss: 0.5820 \tValidation Accuracy: 0.750 \ttime: 72.5 \tlr=0.0001\n",
      "Epoch: 58 \tTraining Loss: 0.5181 \tValidation Loss: 0.5813 \tValidation Accuracy: 0.746 \ttime: 73.8 \tlr=1e-05\n",
      "Epoch: 59 \tTraining Loss: 0.5179 \tValidation Loss: 0.5856 \tValidation Accuracy: 0.740 \ttime: 73.2 \tlr=1e-05\n",
      "Epoch: 60 \tTraining Loss: 0.5130 \tValidation Loss: 0.5812 \tValidation Accuracy: 0.750 \ttime: 72.7 \tlr=1e-05\n",
      "Epoch: 61 \tTraining Loss: 0.5110 \tValidation Loss: 0.5814 \tValidation Accuracy: 0.743 \ttime: 74.3 \tlr=1e-05\n",
      "Epoch: 62 \tTraining Loss: 0.5105 \tValidation Loss: 0.5814 \tValidation Accuracy: 0.743 \ttime: 72.2 \tlr=1e-05\n",
      "Epoch: 63 \tTraining Loss: 0.5104 \tValidation Loss: 0.5815 \tValidation Accuracy: 0.745 \ttime: 73.4 \tlr=1e-05\n",
      "Epoch: 64 \tTraining Loss: 0.5216 \tValidation Loss: 0.5837 \tValidation Accuracy: 0.743 \ttime: 71.5 \tlr=1e-05\n",
      "Epoch: 65 \tTraining Loss: 0.5200 \tValidation Loss: 0.5832 \tValidation Accuracy: 0.745 \ttime: 73.6 \tlr=1e-05\n",
      "Epoch: 66 \tTraining Loss: 0.5158 \tValidation Loss: 0.5838 \tValidation Accuracy: 0.751 \ttime: 73.4 \tlr=1e-05\n",
      "Epoch: 67 \tTraining Loss: 0.5228 \tValidation Loss: 0.5845 \tValidation Accuracy: 0.741 \ttime: 73.0 \tlr=1e-05\n",
      "Epoch: 68 \tTraining Loss: 0.5138 \tValidation Loss: 0.5827 \tValidation Accuracy: 0.745 \ttime: 73.5 \tlr=1.0000000000000002e-06\n",
      "Epoch: 69 \tTraining Loss: 0.5182 \tValidation Loss: 0.5846 \tValidation Accuracy: 0.746 \ttime: 73.9 \tlr=1.0000000000000002e-06\n",
      "Epoch: 70 \tTraining Loss: 0.5275 \tValidation Loss: 0.5865 \tValidation Accuracy: 0.745 \ttime: 72.9 \tlr=1.0000000000000002e-06\n",
      "Epoch: 71 \tTraining Loss: 0.5140 \tValidation Loss: 0.5793 \tValidation Accuracy: 0.743 \ttime: 73.4 \tlr=1.0000000000000002e-06\n",
      "Epoch: 72 \tTraining Loss: 0.5143 \tValidation Loss: 0.5811 \tValidation Accuracy: 0.745 \ttime: 72.8 \tlr=1.0000000000000002e-06\n",
      "Epoch: 73 \tTraining Loss: 0.5161 \tValidation Loss: 0.5811 \tValidation Accuracy: 0.748 \ttime: 71.8 \tlr=1.0000000000000002e-06\n",
      "Epoch: 74 \tTraining Loss: 0.5224 \tValidation Loss: 0.5824 \tValidation Accuracy: 0.748 \ttime: 74.4 \tlr=1.0000000000000002e-06\n",
      "Epoch: 75 \tTraining Loss: 0.5162 \tValidation Loss: 0.5812 \tValidation Accuracy: 0.741 \ttime: 72.0 \tlr=1.0000000000000002e-06\n",
      "Epoch: 76 \tTraining Loss: 0.5119 \tValidation Loss: 0.5818 \tValidation Accuracy: 0.743 \ttime: 72.7 \tlr=1.0000000000000002e-06\n",
      "Epoch: 77 \tTraining Loss: 0.5097 \tValidation Loss: 0.5810 \tValidation Accuracy: 0.745 \ttime: 72.1 \tlr=1.0000000000000002e-06\n",
      "Epoch: 78 \tTraining Loss: 0.5152 \tValidation Loss: 0.5813 \tValidation Accuracy: 0.748 \ttime: 72.8 \tlr=1.0000000000000002e-07\n",
      "Epoch: 79 \tTraining Loss: 0.5189 \tValidation Loss: 0.5826 \tValidation Accuracy: 0.746 \ttime: 72.4 \tlr=1.0000000000000002e-07\n",
      "Epoch: 80 \tTraining Loss: 0.5115 \tValidation Loss: 0.5829 \tValidation Accuracy: 0.744 \ttime: 73.9 \tlr=1.0000000000000002e-07\n",
      "Epoch: 81 \tTraining Loss: 0.5183 \tValidation Loss: 0.5852 \tValidation Accuracy: 0.745 \ttime: 75.0 \tlr=1.0000000000000002e-07\n",
      "Epoch: 82 \tTraining Loss: 0.5231 \tValidation Loss: 0.5830 \tValidation Accuracy: 0.743 \ttime: 73.5 \tlr=1.0000000000000002e-07\n",
      "Epoch: 83 \tTraining Loss: 0.5128 \tValidation Loss: 0.5846 \tValidation Accuracy: 0.744 \ttime: 72.5 \tlr=1.0000000000000002e-07\n",
      "Epoch: 84 \tTraining Loss: 0.5111 \tValidation Loss: 0.5825 \tValidation Accuracy: 0.743 \ttime: 70.2 \tlr=1.0000000000000002e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1014:\n",
      "Process Process-1011:\n",
      "Process Process-1012:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1009:\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Process Process-1013:\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 103, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/Image.py\", line 915, in convert\n",
      "    self.load()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/ImageFile.py\", line 241, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "Process Process-1010:\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n",
      "    return F.resize(img, self.size, self.interpolation)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/Image.py\", line 915, in convert\n",
      "    self.load()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/ImageFile.py\", line 241, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 204, in resize\n",
      "    return img.resize((ow, oh), interpolation)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/Image.py\", line 915, in convert\n",
      "    self.load()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/ImageFile.py\", line 241, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/Image.py\", line 915, in convert\n",
      "    self.load()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/ImageFile.py\", line 241, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/Image.py\", line 1806, in resize\n",
      "    return self._new(self.im.resize(size, resample, box))\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/Image.py\", line 915, in convert\n",
      "    self.load()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/PIL/ImageFile.py\", line 241, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-c21dfb8596a2>\", line 95, in <module>\n",
      "    MODEL_WEIGHTS_FILE)\n",
      "  File \"<ipython-input-10-c21dfb8596a2>\", line 26, in train\n",
      "    for batch_idx, (data, target) in enumerate(loaders_transfer['train']):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 280, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 259, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 25810) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# train the model\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, save_path):\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    # Valid Loss Stable counter\n",
    "    valid_loss_stable_counter = 0\n",
    "\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "                \n",
    "        start = time.time()\n",
    "        \n",
    "        lr = get_lr_from_optim(optimizer)\n",
    "        \n",
    "        # train the model\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # validate the model\n",
    "\n",
    "        model.eval()\n",
    "        valid_corrects = 0\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['valid']):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "            valid_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "        # ---------\n",
    "        \n",
    "        train_loss = train_loss/len(loaders_transfer['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders_transfer['valid'].dataset)\n",
    "        \n",
    "        epoch_acc = valid_corrects.double() / len(loaders_transfer['valid'].dataset)\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.3f} \\ttime: {:.1f} \\tlr={}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            epoch_acc,\n",
    "            time.time() - start,\n",
    "            lr\n",
    "            ))\n",
    "        \n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, save_path)\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "            valid_loss_stable_counter = 0\n",
    "        else:\n",
    "            valid_loss_stable_counter += 1\n",
    "            if valid_loss_stable_counter >= valid_loss_stable_count:\n",
    "                valid_loss_stable_counter = 0\n",
    "                lr = get_lr_from_optim(optimizer)\n",
    "                lr = lr * lr_decay_factor\n",
    "                if lr <= lr_lower_bound:\n",
    "                    return model\n",
    "                set_lr_to_optim(optimizer, lr)\n",
    "                \n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "train_start = time.time()\n",
    "model_transfer = train(num_training_epochs,\n",
    "                       loaders_transfer,\n",
    "                       model_transfer,optimizer_transfer,\n",
    "                       criterion_transfer,\n",
    "                       MODEL_WEIGHTS_FILE)\n",
    "print(\"Total training time: {:.2f} seconds\".format(time.time() - train_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following module can be run separately if trained weights are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ToChange!!!\n",
    "# MODEL_WEIGHTS_FILE = 'model_weights_v3_1__run_1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.models as models\n",
    "# import torch.nn as nn\n",
    "# import torchvision.datasets\n",
    "# import torchvision.transforms as transforms\n",
    "# from PIL import ImageFile\n",
    "\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "# model_transfer.fc = nn.Sequential(\n",
    "#                nn.Linear(2048, 128),\n",
    "#                nn.ReLU(inplace=True),\n",
    "#                nn.Linear(128, 3)).to(device)\n",
    "# model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_DATA_PATH = '../data/test'\n",
    "# transform_plain = transforms.Compose([\n",
    "#                             transforms.Resize(224),\n",
    "#                             transforms.CenterCrop(224),\n",
    "#                             transforms.ToTensor(),\n",
    "#                             transforms.Normalize(\n",
    "#                                     mean=(0.485, 0.456, 0.406),\n",
    "#                                     std =(0.229, 0.224, 0.225))\n",
    "#                            ]) \n",
    "\n",
    "# if not 'data_transfer' in locals():\n",
    "#     print(\"create empty data_transfer\")\n",
    "#     data_transfer = {}\n",
    "# if not 'loaders_transfer' in locals():\n",
    "#     print(\"create empty loaders_transfer\")\n",
    "#     loaders_transfer = {}\n",
    "# data_transfer['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "# loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "#                                           batch_size=1,\n",
    "#                                           shuffle=False,\n",
    "#                                           num_workers=4)\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# criterion_transfer = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.678148\n",
      "\n",
      "\n",
      "Test Accuracy: 70% (140/200)\n",
      "Total testing time: 4.64 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def test(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # accumulate test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "        # compare predictions to true label\n",
    "        if torch.cuda.is_available():\n",
    "            correct += torch.sum(preds == target.data)\n",
    "        else:\n",
    "            correct += np.sum(np.squeeze(preds.eq(target.data.view_as(preds))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "        \n",
    "    test_loss = test_loss/len(loaders_transfer['test'].dataset)      \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function   \n",
    "test_start = time.time()\n",
    "test(loaders_transfer, model_transfer, criterion_transfer)\n",
    "print(\"Total testing time: {:.2f} seconds\".format(time.time() - test_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
