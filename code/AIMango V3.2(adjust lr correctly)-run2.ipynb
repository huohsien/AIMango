{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#### batch_size = 128\n",
    "#### num_training_epochs = 400\n",
    "#### lr = 0.01\n",
    "#### valid_loss_stable_count = 15\n",
    "\n",
    "#### Make learning rate lr one tenth if the number of epochs in which validation loss doesn't decrease exceeds the paramter of valid_loss_stable_count.\n",
    "\n",
    "#### Test Accuracy: 72% (144/200)\n",
    "#### Total testing time: 4.52 seconds \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_CSV_PATH = '../C1-P1_Train Dev_fixed/train.csv'\n",
    "TRAIN_CSV_PATH = '../C1-P1_Train Dev_fixed/train_split.csv'\n",
    "VALID_CSV_PATH = '../C1-P1_Train Dev_fixed/dev.csv'\n",
    "\n",
    "ORIGINAL_TRAIN_TEST_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "ORIGINAL_VALID_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Dev/' \n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train'\n",
    "VALID_DATA_PATH = '../data/valid'\n",
    "TEST_DATA_PATH = '../data/test'\n",
    "\n",
    "# ToChange!!!\n",
    "MODEL_WEIGHTS_FILE = 'model_weights_v3_2_run_2.pt'\n",
    "\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "valid_loss_stable_count = 15  # and when valid loss is stable, adjust lr\n",
    "lr_decay_factor = 0.1\n",
    "lr_lower_bound = 1e-4\n",
    "num_training_epochs = 400\n",
    "\n",
    "num_worker = 6\n",
    "sgd_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(degrees=(-15, 15)),\n",
    "                                transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "transform_plain = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                           ]) \n",
    "\n",
    "loaders_transfer = {}\n",
    "data_transfer = {}\n",
    "\n",
    "data_transfer['train'] = torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, transform=transform)\n",
    "loaders_transfer['train'] = torch.utils.data.DataLoader(data_transfer['train'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_worker)\n",
    "\n",
    "data_transfer['valid'] = torchvision.datasets.ImageFolder(VALID_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['valid'] = torch.utils.data.DataLoader(data_transfer['valid'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_worker)\n",
    "data_transfer['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model_transfer.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 3)).to(device)\n",
    "## uncomment the following line for continuing trainging only\n",
    "# model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.SGD(model_transfer.fc.parameters(), lr = lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_from_optim(optimizer):\n",
    "    for param_group in optimizer_transfer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr_to_optim(optimizer, lr):\n",
    "    for param_group in optimizer_transfer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.9086 \tValidation Loss: 0.7990 \tValidation Accuracy: 0.611 \ttime: 70.7 \tlr=0.01\n",
      "Validation loss decreased from inf to 0.799014. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 0.7067 \tValidation Loss: 0.7282 \tValidation Accuracy: 0.670 \ttime: 71.3 \tlr=0.01\n",
      "Validation loss decreased from 0.799014 to 0.728174. Model was saved\n",
      "Epoch: 3 \tTraining Loss: 0.6946 \tValidation Loss: 0.7498 \tValidation Accuracy: 0.660 \ttime: 72.7 \tlr=0.01\n",
      "Epoch: 4 \tTraining Loss: 0.6648 \tValidation Loss: 0.6433 \tValidation Accuracy: 0.725 \ttime: 71.8 \tlr=0.01\n",
      "Validation loss decreased from 0.728174 to 0.643282. Model was saved\n",
      "Epoch: 5 \tTraining Loss: 0.6852 \tValidation Loss: 0.6766 \tValidation Accuracy: 0.695 \ttime: 73.4 \tlr=0.01\n",
      "Epoch: 6 \tTraining Loss: 0.6718 \tValidation Loss: 0.6392 \tValidation Accuracy: 0.729 \ttime: 74.6 \tlr=0.01\n",
      "Validation loss decreased from 0.643282 to 0.639175. Model was saved\n",
      "Epoch: 7 \tTraining Loss: 0.6255 \tValidation Loss: 0.7054 \tValidation Accuracy: 0.680 \ttime: 73.9 \tlr=0.01\n",
      "Epoch: 8 \tTraining Loss: 0.6398 \tValidation Loss: 0.6307 \tValidation Accuracy: 0.719 \ttime: 72.5 \tlr=0.01\n",
      "Validation loss decreased from 0.639175 to 0.630688. Model was saved\n",
      "Epoch: 9 \tTraining Loss: 0.6538 \tValidation Loss: 0.7232 \tValidation Accuracy: 0.675 \ttime: 72.8 \tlr=0.01\n",
      "Epoch: 10 \tTraining Loss: 0.6199 \tValidation Loss: 0.6419 \tValidation Accuracy: 0.718 \ttime: 73.2 \tlr=0.01\n",
      "Epoch: 11 \tTraining Loss: 0.6064 \tValidation Loss: 0.6530 \tValidation Accuracy: 0.698 \ttime: 73.5 \tlr=0.01\n",
      "Epoch: 12 \tTraining Loss: 0.6105 \tValidation Loss: 0.7141 \tValidation Accuracy: 0.686 \ttime: 74.3 \tlr=0.01\n",
      "Epoch: 13 \tTraining Loss: 0.6091 \tValidation Loss: 0.6171 \tValidation Accuracy: 0.733 \ttime: 74.6 \tlr=0.01\n",
      "Validation loss decreased from 0.630688 to 0.617124. Model was saved\n",
      "Epoch: 14 \tTraining Loss: 0.6012 \tValidation Loss: 0.6035 \tValidation Accuracy: 0.740 \ttime: 73.8 \tlr=0.01\n",
      "Validation loss decreased from 0.617124 to 0.603536. Model was saved\n",
      "Epoch: 15 \tTraining Loss: 0.5991 \tValidation Loss: 0.6139 \tValidation Accuracy: 0.738 \ttime: 73.8 \tlr=0.01\n",
      "Epoch: 16 \tTraining Loss: 0.5999 \tValidation Loss: 0.6055 \tValidation Accuracy: 0.736 \ttime: 74.8 \tlr=0.01\n",
      "Epoch: 17 \tTraining Loss: 0.5854 \tValidation Loss: 0.6800 \tValidation Accuracy: 0.699 \ttime: 72.7 \tlr=0.01\n",
      "Epoch: 18 \tTraining Loss: 0.6027 \tValidation Loss: 0.6070 \tValidation Accuracy: 0.745 \ttime: 71.8 \tlr=0.01\n",
      "Epoch: 19 \tTraining Loss: 0.6005 \tValidation Loss: 0.6307 \tValidation Accuracy: 0.720 \ttime: 73.5 \tlr=0.01\n",
      "Epoch: 20 \tTraining Loss: 0.5939 \tValidation Loss: 0.6111 \tValidation Accuracy: 0.719 \ttime: 72.0 \tlr=0.01\n",
      "Epoch: 21 \tTraining Loss: 0.5801 \tValidation Loss: 0.6058 \tValidation Accuracy: 0.731 \ttime: 73.8 \tlr=0.01\n",
      "Epoch: 22 \tTraining Loss: 0.5785 \tValidation Loss: 0.6062 \tValidation Accuracy: 0.744 \ttime: 73.4 \tlr=0.01\n",
      "Epoch: 23 \tTraining Loss: 0.5794 \tValidation Loss: 0.5902 \tValidation Accuracy: 0.740 \ttime: 73.4 \tlr=0.01\n",
      "Validation loss decreased from 0.603536 to 0.590220. Model was saved\n",
      "Epoch: 24 \tTraining Loss: 0.5827 \tValidation Loss: 0.5939 \tValidation Accuracy: 0.751 \ttime: 72.7 \tlr=0.01\n",
      "Epoch: 25 \tTraining Loss: 0.5804 \tValidation Loss: 0.6035 \tValidation Accuracy: 0.744 \ttime: 71.7 \tlr=0.01\n",
      "Epoch: 26 \tTraining Loss: 0.5741 \tValidation Loss: 0.6340 \tValidation Accuracy: 0.731 \ttime: 69.4 \tlr=0.01\n",
      "Epoch: 27 \tTraining Loss: 0.5724 \tValidation Loss: 0.6027 \tValidation Accuracy: 0.745 \ttime: 71.5 \tlr=0.01\n",
      "Epoch: 28 \tTraining Loss: 0.5708 \tValidation Loss: 0.6360 \tValidation Accuracy: 0.733 \ttime: 72.3 \tlr=0.01\n",
      "Epoch: 29 \tTraining Loss: 0.5778 \tValidation Loss: 0.5930 \tValidation Accuracy: 0.743 \ttime: 73.1 \tlr=0.01\n",
      "Epoch: 30 \tTraining Loss: 0.5608 \tValidation Loss: 0.6136 \tValidation Accuracy: 0.738 \ttime: 71.9 \tlr=0.01\n",
      "Epoch: 31 \tTraining Loss: 0.5849 \tValidation Loss: 0.6339 \tValidation Accuracy: 0.723 \ttime: 73.4 \tlr=0.01\n",
      "Epoch: 32 \tTraining Loss: 0.5449 \tValidation Loss: 0.6085 \tValidation Accuracy: 0.740 \ttime: 71.4 \tlr=0.01\n",
      "Epoch: 33 \tTraining Loss: 0.5711 \tValidation Loss: 0.5902 \tValidation Accuracy: 0.750 \ttime: 73.7 \tlr=0.01\n",
      "Validation loss decreased from 0.590220 to 0.590208. Model was saved\n",
      "Epoch: 34 \tTraining Loss: 0.5479 \tValidation Loss: 0.6050 \tValidation Accuracy: 0.736 \ttime: 72.2 \tlr=0.01\n",
      "Epoch: 35 \tTraining Loss: 0.5547 \tValidation Loss: 0.5726 \tValidation Accuracy: 0.744 \ttime: 73.1 \tlr=0.01\n",
      "Validation loss decreased from 0.590208 to 0.572623. Model was saved\n",
      "Epoch: 36 \tTraining Loss: 0.5461 \tValidation Loss: 0.6011 \tValidation Accuracy: 0.738 \ttime: 74.3 \tlr=0.01\n",
      "Epoch: 37 \tTraining Loss: 0.5540 \tValidation Loss: 0.5864 \tValidation Accuracy: 0.751 \ttime: 73.5 \tlr=0.01\n",
      "Epoch: 38 \tTraining Loss: 0.5561 \tValidation Loss: 0.5988 \tValidation Accuracy: 0.734 \ttime: 72.6 \tlr=0.01\n",
      "Epoch: 39 \tTraining Loss: 0.5625 \tValidation Loss: 0.6230 \tValidation Accuracy: 0.736 \ttime: 73.7 \tlr=0.01\n",
      "Epoch: 40 \tTraining Loss: 0.5342 \tValidation Loss: 0.6116 \tValidation Accuracy: 0.729 \ttime: 73.4 \tlr=0.01\n",
      "Epoch: 41 \tTraining Loss: 0.5315 \tValidation Loss: 0.5926 \tValidation Accuracy: 0.743 \ttime: 71.7 \tlr=0.01\n",
      "Epoch: 42 \tTraining Loss: 0.5368 \tValidation Loss: 0.5853 \tValidation Accuracy: 0.739 \ttime: 73.6 \tlr=0.01\n",
      "Epoch: 43 \tTraining Loss: 0.5554 \tValidation Loss: 0.6432 \tValidation Accuracy: 0.710 \ttime: 72.6 \tlr=0.01\n",
      "Epoch: 44 \tTraining Loss: 0.5469 \tValidation Loss: 0.5741 \tValidation Accuracy: 0.750 \ttime: 74.8 \tlr=0.01\n",
      "Epoch: 45 \tTraining Loss: 0.5266 \tValidation Loss: 0.5695 \tValidation Accuracy: 0.760 \ttime: 71.6 \tlr=0.01\n",
      "Validation loss decreased from 0.572623 to 0.569476. Model was saved\n",
      "Epoch: 46 \tTraining Loss: 0.5205 \tValidation Loss: 0.5960 \tValidation Accuracy: 0.743 \ttime: 73.6 \tlr=0.01\n",
      "Epoch: 47 \tTraining Loss: 0.5199 \tValidation Loss: 0.5807 \tValidation Accuracy: 0.749 \ttime: 72.8 \tlr=0.01\n",
      "Epoch: 48 \tTraining Loss: 0.5516 \tValidation Loss: 0.6186 \tValidation Accuracy: 0.734 \ttime: 74.5 \tlr=0.01\n",
      "Epoch: 49 \tTraining Loss: 0.5285 \tValidation Loss: 0.5907 \tValidation Accuracy: 0.738 \ttime: 72.5 \tlr=0.01\n",
      "Epoch: 50 \tTraining Loss: 0.5258 \tValidation Loss: 0.6176 \tValidation Accuracy: 0.728 \ttime: 71.6 \tlr=0.01\n",
      "Epoch: 51 \tTraining Loss: 0.5186 \tValidation Loss: 0.5733 \tValidation Accuracy: 0.756 \ttime: 72.1 \tlr=0.01\n",
      "Epoch: 52 \tTraining Loss: 0.5210 \tValidation Loss: 0.6066 \tValidation Accuracy: 0.741 \ttime: 72.8 \tlr=0.01\n",
      "Epoch: 53 \tTraining Loss: 0.5349 \tValidation Loss: 0.6115 \tValidation Accuracy: 0.736 \ttime: 74.0 \tlr=0.01\n",
      "Epoch: 54 \tTraining Loss: 0.5103 \tValidation Loss: 0.6041 \tValidation Accuracy: 0.734 \ttime: 72.8 \tlr=0.01\n",
      "Epoch: 55 \tTraining Loss: 0.5268 \tValidation Loss: 0.6548 \tValidation Accuracy: 0.728 \ttime: 73.7 \tlr=0.01\n",
      "Epoch: 56 \tTraining Loss: 0.5216 \tValidation Loss: 0.5821 \tValidation Accuracy: 0.745 \ttime: 72.6 \tlr=0.01\n",
      "Epoch: 57 \tTraining Loss: 0.5154 \tValidation Loss: 0.6027 \tValidation Accuracy: 0.736 \ttime: 75.3 \tlr=0.01\n",
      "Epoch: 58 \tTraining Loss: 0.5172 \tValidation Loss: 0.5663 \tValidation Accuracy: 0.756 \ttime: 74.6 \tlr=0.01\n",
      "Validation loss decreased from 0.569476 to 0.566261. Model was saved\n",
      "Epoch: 59 \tTraining Loss: 0.5040 \tValidation Loss: 0.5813 \tValidation Accuracy: 0.753 \ttime: 73.4 \tlr=0.01\n",
      "Epoch: 60 \tTraining Loss: 0.5080 \tValidation Loss: 0.6238 \tValidation Accuracy: 0.741 \ttime: 74.8 \tlr=0.01\n",
      "Epoch: 61 \tTraining Loss: 0.5113 \tValidation Loss: 0.6038 \tValidation Accuracy: 0.734 \ttime: 74.6 \tlr=0.01\n",
      "Epoch: 62 \tTraining Loss: 0.5186 \tValidation Loss: 0.6148 \tValidation Accuracy: 0.741 \ttime: 73.0 \tlr=0.01\n",
      "Epoch: 63 \tTraining Loss: 0.5206 \tValidation Loss: 0.6003 \tValidation Accuracy: 0.746 \ttime: 73.7 \tlr=0.01\n",
      "Epoch: 64 \tTraining Loss: 0.5084 \tValidation Loss: 0.5788 \tValidation Accuracy: 0.760 \ttime: 72.7 \tlr=0.01\n",
      "Epoch: 65 \tTraining Loss: 0.5093 \tValidation Loss: 0.6723 \tValidation Accuracy: 0.714 \ttime: 73.2 \tlr=0.01\n",
      "Epoch: 66 \tTraining Loss: 0.5196 \tValidation Loss: 0.5947 \tValidation Accuracy: 0.745 \ttime: 74.6 \tlr=0.01\n",
      "Epoch: 67 \tTraining Loss: 0.5105 \tValidation Loss: 0.5896 \tValidation Accuracy: 0.753 \ttime: 73.5 \tlr=0.01\n",
      "Epoch: 68 \tTraining Loss: 0.4784 \tValidation Loss: 0.5734 \tValidation Accuracy: 0.765 \ttime: 74.6 \tlr=0.01\n",
      "Epoch: 69 \tTraining Loss: 0.4833 \tValidation Loss: 0.6276 \tValidation Accuracy: 0.741 \ttime: 73.2 \tlr=0.01\n",
      "Epoch: 70 \tTraining Loss: 0.4838 \tValidation Loss: 0.5928 \tValidation Accuracy: 0.748 \ttime: 73.3 \tlr=0.01\n",
      "Epoch: 71 \tTraining Loss: 0.4747 \tValidation Loss: 0.6015 \tValidation Accuracy: 0.744 \ttime: 73.2 \tlr=0.01\n",
      "Epoch: 72 \tTraining Loss: 0.4771 \tValidation Loss: 0.7070 \tValidation Accuracy: 0.689 \ttime: 72.9 \tlr=0.01\n",
      "Epoch: 73 \tTraining Loss: 0.4978 \tValidation Loss: 0.6026 \tValidation Accuracy: 0.745 \ttime: 72.3 \tlr=0.01\n",
      "Epoch: 74 \tTraining Loss: 0.4505 \tValidation Loss: 0.5807 \tValidation Accuracy: 0.751 \ttime: 72.8 \tlr=0.001\n",
      "Epoch: 75 \tTraining Loss: 0.4491 \tValidation Loss: 0.5834 \tValidation Accuracy: 0.758 \ttime: 72.5 \tlr=0.001\n",
      "Epoch: 76 \tTraining Loss: 0.4399 \tValidation Loss: 0.5941 \tValidation Accuracy: 0.745 \ttime: 73.0 \tlr=0.001\n",
      "Epoch: 77 \tTraining Loss: 0.4529 \tValidation Loss: 0.5869 \tValidation Accuracy: 0.751 \ttime: 73.3 \tlr=0.001\n",
      "Epoch: 78 \tTraining Loss: 0.4378 \tValidation Loss: 0.5845 \tValidation Accuracy: 0.753 \ttime: 73.0 \tlr=0.001\n",
      "Epoch: 79 \tTraining Loss: 0.4454 \tValidation Loss: 0.5976 \tValidation Accuracy: 0.750 \ttime: 73.7 \tlr=0.001\n",
      "Epoch: 80 \tTraining Loss: 0.4474 \tValidation Loss: 0.5839 \tValidation Accuracy: 0.750 \ttime: 72.9 \tlr=0.001\n",
      "Epoch: 81 \tTraining Loss: 0.4406 \tValidation Loss: 0.5866 \tValidation Accuracy: 0.751 \ttime: 73.4 \tlr=0.001\n",
      "Epoch: 82 \tTraining Loss: 0.4351 \tValidation Loss: 0.5919 \tValidation Accuracy: 0.754 \ttime: 72.3 \tlr=0.001\n",
      "Epoch: 83 \tTraining Loss: 0.4419 \tValidation Loss: 0.5813 \tValidation Accuracy: 0.745 \ttime: 73.7 \tlr=0.001\n",
      "Epoch: 84 \tTraining Loss: 0.4384 \tValidation Loss: 0.5797 \tValidation Accuracy: 0.755 \ttime: 72.8 \tlr=0.001\n",
      "Epoch: 85 \tTraining Loss: 0.4372 \tValidation Loss: 0.5817 \tValidation Accuracy: 0.756 \ttime: 72.6 \tlr=0.001\n",
      "Epoch: 86 \tTraining Loss: 0.4361 \tValidation Loss: 0.5822 \tValidation Accuracy: 0.751 \ttime: 72.4 \tlr=0.001\n",
      "Epoch: 87 \tTraining Loss: 0.4332 \tValidation Loss: 0.5901 \tValidation Accuracy: 0.749 \ttime: 72.3 \tlr=0.001\n",
      "Epoch: 88 \tTraining Loss: 0.4308 \tValidation Loss: 0.5889 \tValidation Accuracy: 0.753 \ttime: 73.9 \tlr=0.001\n",
      "Total training time: 6454.92 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# train the model\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, save_path):\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    # Valid Loss Stable counter\n",
    "    valid_loss_stable_counter = 0\n",
    "\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "                \n",
    "        start = time.time()\n",
    "        \n",
    "        lr = get_lr_from_optim(optimizer)\n",
    "        \n",
    "        # train the model\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # validate the model\n",
    "\n",
    "        model.eval()\n",
    "        valid_corrects = 0\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['valid']):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "            valid_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "        # ---------\n",
    "        \n",
    "        train_loss = train_loss/len(loaders_transfer['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders_transfer['valid'].dataset)\n",
    "        \n",
    "        epoch_acc = valid_corrects.double() / len(loaders_transfer['valid'].dataset)\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.3f} \\ttime: {:.1f} \\tlr={}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            epoch_acc,\n",
    "            time.time() - start,\n",
    "            lr\n",
    "            ))\n",
    "        \n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, save_path)\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "            valid_loss_stable_counter = 0\n",
    "        else:\n",
    "            valid_loss_stable_counter += 1\n",
    "            if valid_loss_stable_counter >= valid_loss_stable_count:\n",
    "                valid_loss_stable_counter = 0\n",
    "                lr = get_lr_from_optim(optimizer)\n",
    "                lr = lr * lr_decay_factor\n",
    "                if lr <= lr_lower_bound:\n",
    "                    return model\n",
    "                set_lr_to_optim(optimizer, lr)\n",
    "                \n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "train_start = time.time()\n",
    "model_transfer = train(num_training_epochs,\n",
    "                       loaders_transfer,\n",
    "                       model_transfer,optimizer_transfer,\n",
    "                       criterion_transfer,\n",
    "                       MODEL_WEIGHTS_FILE)\n",
    "print(\"Total training time: {:.2f} seconds\".format(time.time() - train_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following module can be run separately if trained weights are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ToChange!!!\n",
    "# MODEL_WEIGHTS_FILE = 'model_weights_v3_1__run_1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.models as models\n",
    "# import torch.nn as nn\n",
    "# import torchvision.datasets\n",
    "# import torchvision.transforms as transforms\n",
    "# from PIL import ImageFile\n",
    "\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "# model_transfer.fc = nn.Sequential(\n",
    "#                nn.Linear(2048, 128),\n",
    "#                nn.ReLU(inplace=True),\n",
    "#                nn.Linear(128, 3)).to(device)\n",
    "# model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_DATA_PATH = '../data/test'\n",
    "# transform_plain = transforms.Compose([\n",
    "#                             transforms.Resize(224),\n",
    "#                             transforms.CenterCrop(224),\n",
    "#                             transforms.ToTensor(),\n",
    "#                             transforms.Normalize(\n",
    "#                                     mean=(0.485, 0.456, 0.406),\n",
    "#                                     std =(0.229, 0.224, 0.225))\n",
    "#                            ]) \n",
    "\n",
    "# if not 'data_transfer' in locals():\n",
    "#     print(\"create empty data_transfer\")\n",
    "#     data_transfer = {}\n",
    "# if not 'loaders_transfer' in locals():\n",
    "#     print(\"create empty loaders_transfer\")\n",
    "#     loaders_transfer = {}\n",
    "# data_transfer['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "# loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "#                                           batch_size=1,\n",
    "#                                           shuffle=False,\n",
    "#                                           num_workers=4)\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# criterion_transfer = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the weight that give the lowest loss\n",
    "model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.687623\n",
      "\n",
      "\n",
      "Test Accuracy: 72% (144/200)\n",
      "Total testing time: 4.52 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def test(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # accumulate test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "        # compare predictions to true label\n",
    "        if torch.cuda.is_available():\n",
    "            correct += torch.sum(preds == target.data)\n",
    "        else:\n",
    "            correct += np.sum(np.squeeze(preds.eq(target.data.view_as(preds))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "        \n",
    "    test_loss = test_loss/len(loaders_transfer['test'].dataset)      \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function   \n",
    "test_start = time.time()\n",
    "test(loaders_transfer, model_transfer, criterion_transfer)\n",
    "print(\"Total testing time: {:.2f} seconds\".format(time.time() - test_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
