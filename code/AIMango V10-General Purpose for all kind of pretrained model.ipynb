{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import sklearn.model_selection\n",
    "\n",
    "from shutil import copyfile\n",
    "from PIL import ImageFile\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#-----\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "#-----\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_CSV_PATH = '../C1-P1_Train Dev_fixed/train.csv'\n",
    "TRAIN_CSV_PATH = '../C1-P1_Train Dev_fixed/train_split.csv'\n",
    "VALID_CSV_PATH = '../C1-P1_Train Dev_fixed/dev.csv'\n",
    "\n",
    "ORIGINAL_TRAIN_TEST_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "# ORIGINAL_TRAIN_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "ORIGINAL_VALID_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Dev/' \n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train'\n",
    "VALID_DATA_PATH = '../data/valid'\n",
    "TEST_DATA_PATH = '../data/test'\n",
    "\n",
    "TRAINED_MODEL_WEIGHTS_FILE_NAME = 'model_v4_weights.pt'\n",
    "NUM_TEST_DATASET = 100\n",
    "\n",
    "image_size = 299\n",
    "# image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "valid_loss_stable_count = 14\n",
    "num_training_epochs = 100\n",
    "exp_lr_scheduler_step_size = 7\n",
    "exp_lr_scheduler_gamma = 0.1\n",
    "sgd_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can choose not to use  data in \"../AIMango_sample/\" as test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use data in \"../AIMango_sample/\" as test dataset for evaluation\n",
    "# # the format of this dataset is different from the rest\n",
    "# # the csv file looks like: D-Plant2_0610_3.jpg,等級B\n",
    "# # the following code is to transform it into: D-Plant2_0610_3.jpg,B\n",
    "\n",
    "# TEST_CSV_PATH = '../AIMango_sample/label.csv'\n",
    "# ORIGINAL_TEST_DATA_PATH = '../AIMango_sample/sample_image/' \n",
    "\n",
    "# new_data = []\n",
    "# with open(TEST_CSV_PATH) as f:\n",
    "#     reader = csv.reader(f, delimiter=',')\n",
    "#     for row in reader:\n",
    "#         new_label = row[1][len(row[1])-1] # remove 等級 from the label 等級B and get the new lable B\n",
    "#         new_data.append([row[0],new_label])\n",
    "\n",
    "\n",
    "# # save the data from the original csv file that was transformed by the code above to a new csv file\n",
    "# folder, filename = os.path.split(TEST_CSV_PATH)\n",
    "# TEST_CSV_PATH = os.path.join(folder, 'label_new.csv')\n",
    "                                 \n",
    "# with open(TEST_CSV_PATH, 'w') as f:\n",
    "\n",
    "#     writer = csv.writer(f)\n",
    "    \n",
    "#     for row in new_data:\n",
    "#         writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data from train dataset. get last 100 records as test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = []\n",
    "new_test_data = []\n",
    "all_data = []\n",
    "\n",
    "TEST_CSV_PATH = '../C1-P1_Train Dev_fixed/test_split.csv'\n",
    "\n",
    "with open(TRAIN_TEST_CSV_PATH) as csv_file:\n",
    "\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue  #header\n",
    "        all_data.append(row)\n",
    "        \n",
    "        line_count += 1\n",
    "        \n",
    "indices = list(range(len(all_data)))\n",
    "\n",
    "random.shuffle(indices)\n",
    "training_dataset, test_dataset = sklearn.model_selection.train_test_split(indices, train_size=len(all_data)-NUM_TEST_DATASET, test_size=NUM_TEST_DATASET)\n",
    "\n",
    "for idx in training_dataset:\n",
    "    new_train_data.append(all_data[idx])\n",
    "for idx in test_dataset:\n",
    "    new_test_data.append(all_data[idx])  \n",
    "\n",
    "with open(TRAIN_CSV_PATH, 'w') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(['image_id','label'])\n",
    "    for row in new_train_data:\n",
    "        writer.writerow(row)\n",
    "        \n",
    "with open(TEST_CSV_PATH, 'w') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(['image_id','label'])\n",
    "    for row in new_test_data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse csv file and make the folder structure for pytorch test dataset\n",
    "def prepare_file_structure_for_pytoch(csv_path, src_data_path, dst_data_path):\n",
    "    with open(csv_path) as csv_file:\n",
    "\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "                continue  #header\n",
    "\n",
    "            src_path = os.path.join(src_data_path, row[0])\n",
    "            dest_path = os.path.join(dst_data_path, row[1], row[0])\n",
    "            dest_folder_path = os.path.join(dst_data_path, row[1])\n",
    "            if not os.path.exists(dest_folder_path):\n",
    "                os.makedirs(dest_folder_path)\n",
    "            if not os.path.isfile(dest_path):\n",
    "                copyfile(src_path, dest_path)\n",
    "        \n",
    "            line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../data/train\n",
    "!rm -rf ../data/test\n",
    "!rm -rf ../data/valid\n",
    "# make file structure for training dataset\n",
    "prepare_file_structure_for_pytoch(TRAIN_CSV_PATH, ORIGINAL_TRAIN_TEST_DATA_PATH, TRAIN_DATA_PATH)\n",
    "    \n",
    "# make file structure for validation dataset\n",
    "prepare_file_structure_for_pytoch(VALID_CSV_PATH, ORIGINAL_VALID_DATA_PATH, VALID_DATA_PATH)\n",
    "\n",
    "# make file structure for test dataset\n",
    "prepare_file_structure_for_pytoch(TEST_CSV_PATH, ORIGINAL_TRAIN_TEST_DATA_PATH, TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "aug_transform = transforms.Compose([\n",
    "                                transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(degrees=(-15, 15)),\n",
    "                                transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "\n",
    "vanila_transform = transforms.Compose([\n",
    "                                transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "\n",
    "loaders = {}\n",
    "datasets = {}\n",
    "\n",
    "datasets['train'] = torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, transform=aug_transform)\n",
    "loaders['train'] = torch.utils.data.DataLoader(datasets['train'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "datasets['valid'] = torchvision.datasets.ImageFolder(VALID_DATA_PATH, transform=vanila_transform)\n",
    "loaders['valid'] = torch.utils.data.DataLoader(datasets['valid'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "# datasets['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=vanila_transform)\n",
    "# loaders['test'] = torch.utils.data.DataLoader(datasets['test'],\n",
    "#                                           batch_size=batch_size,\n",
    "#                                           shuffle=True,\n",
    "#                                           num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL in evaluation\n",
    "\n",
    "model = models.inception_v3(pretrained=True)\n",
    "\n",
    "model.aux_logits=False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=sgd_momentum)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=exp_lr_scheduler_step_size, gamma=exp_lr_scheduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.724174 \tValidation Loss: 0.489709 \tValidation Accuracy: 0.7850 \ttime: 294.9\n",
      "Validation loss decreased from inf to 0.489709. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 0.555607 \tValidation Loss: 0.462594 \tValidation Accuracy: 0.7963 \ttime: 305.3\n",
      "Validation loss decreased from 0.489709 to 0.462594. Model was saved\n",
      "Epoch: 3 \tTraining Loss: 0.511448 \tValidation Loss: 0.454716 \tValidation Accuracy: 0.7950 \ttime: 304.4\n",
      "Validation loss decreased from 0.462594 to 0.454716. Model was saved\n",
      "Epoch: 4 \tTraining Loss: 0.472774 \tValidation Loss: 0.466587 \tValidation Accuracy: 0.8000 \ttime: 292.3\n",
      "Epoch: 5 \tTraining Loss: 0.443472 \tValidation Loss: 0.464231 \tValidation Accuracy: 0.7900 \ttime: 288.2\n",
      "Epoch: 6 \tTraining Loss: 0.406102 \tValidation Loss: 0.466787 \tValidation Accuracy: 0.8137 \ttime: 289.8\n",
      "Epoch: 7 \tTraining Loss: 0.389665 \tValidation Loss: 0.460240 \tValidation Accuracy: 0.7938 \ttime: 293.9\n",
      "Epoch: 8 \tTraining Loss: 0.333617 \tValidation Loss: 0.444305 \tValidation Accuracy: 0.8100 \ttime: 292.8\n",
      "Validation loss decreased from 0.454716 to 0.444305. Model was saved\n",
      "Epoch: 9 \tTraining Loss: 0.308073 \tValidation Loss: 0.445409 \tValidation Accuracy: 0.8213 \ttime: 293.1\n",
      "Epoch: 10 \tTraining Loss: 0.290929 \tValidation Loss: 0.438470 \tValidation Accuracy: 0.8137 \ttime: 293.5\n",
      "Validation loss decreased from 0.444305 to 0.438470. Model was saved\n",
      "Epoch: 11 \tTraining Loss: 0.283775 \tValidation Loss: 0.451310 \tValidation Accuracy: 0.8125 \ttime: 295.3\n",
      "Epoch: 12 \tTraining Loss: 0.262321 \tValidation Loss: 0.455567 \tValidation Accuracy: 0.8025 \ttime: 296.6\n",
      "Epoch: 13 \tTraining Loss: 0.267937 \tValidation Loss: 0.448284 \tValidation Accuracy: 0.8075 \ttime: 297.2\n",
      "Epoch: 14 \tTraining Loss: 0.251058 \tValidation Loss: 0.461602 \tValidation Accuracy: 0.8075 \ttime: 297.3\n",
      "Epoch: 15 \tTraining Loss: 0.255998 \tValidation Loss: 0.462231 \tValidation Accuracy: 0.8037 \ttime: 295.6\n",
      "Epoch: 16 \tTraining Loss: 0.249458 \tValidation Loss: 0.457493 \tValidation Accuracy: 0.8075 \ttime: 294.3\n",
      "Epoch: 17 \tTraining Loss: 0.258665 \tValidation Loss: 0.467004 \tValidation Accuracy: 0.8063 \ttime: 291.8\n",
      "Epoch: 18 \tTraining Loss: 0.250257 \tValidation Loss: 0.477172 \tValidation Accuracy: 0.8050 \ttime: 293.2\n",
      "Epoch: 19 \tTraining Loss: 0.243858 \tValidation Loss: 0.456550 \tValidation Accuracy: 0.8125 \ttime: 298.8\n",
      "Epoch: 20 \tTraining Loss: 0.254508 \tValidation Loss: 0.475870 \tValidation Accuracy: 0.8050 \ttime: 296.8\n",
      "Epoch: 21 \tTraining Loss: 0.240992 \tValidation Loss: 0.464184 \tValidation Accuracy: 0.8050 \ttime: 293.7\n",
      "Epoch: 22 \tTraining Loss: 0.236930 \tValidation Loss: 0.460321 \tValidation Accuracy: 0.8037 \ttime: 289.8\n",
      "Epoch: 23 \tTraining Loss: 0.243202 \tValidation Loss: 0.471148 \tValidation Accuracy: 0.8037 \ttime: 285.7\n",
      "Epoch: 24 \tTraining Loss: 0.238805 \tValidation Loss: 0.464093 \tValidation Accuracy: 0.8013 \ttime: 291.3\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, scheduler, save_path):\n",
    "\n",
    "    best_acc = 0.0 \n",
    "    valid_loss_min = np.Inf \n",
    "    valid_loss_stable_counter = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "                \n",
    "        start = time.time()\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update accumulated training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        valid_corrects = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update accumulated validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "            # accumulate number of the accurate predictions\n",
    "            valid_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "        train_loss = train_loss/len(loaders['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
    "        \n",
    "        epoch_acc = valid_corrects.double() / len(loaders['valid'].dataset)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.4f} \\ttime: {:.1f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            epoch_acc,\n",
    "            time.time() - start\n",
    "            ))\n",
    "        \n",
    "#         if epoch_acc > best_acc:\n",
    "#             print('Accuracy increased from {:.4f} to {:.4f}. Model was saved'.format(\n",
    "#                 best_acc,\n",
    "#                 epoch_acc\n",
    "#             ))\n",
    "\n",
    "#             best_acc = epoch_acc\n",
    "#             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#             torch.save(best_model_wts, save_path)\n",
    "\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, TRAINED_MODEL_WEIGHTS_FILE_NAME)\n",
    "\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "            valid_loss_stable_counter = 0\n",
    "        else:\n",
    "            valid_loss_stable_counter += 1\n",
    "            if valid_loss_stable_counter >= valid_loss_stable_count:\n",
    "                # early stop\n",
    "                return model\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "model = train(num_training_epochs,\n",
    "                       loaders,\n",
    "                       model,\n",
    "                       optimizer,\n",
    "                       criterion,\n",
    "                       exp_lr_scheduler,\n",
    "                       TRAINED_MODEL_WEIGHTS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following module can be run separately if trained weights are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The block below define the Model and should be exactly the same as the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.inception_v3(pretrained=True)\n",
    "\n",
    "model.aux_logits=False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model weights that got the best validation accuracy\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "    \n",
    "weights = torch.load(TRAINED_MODEL_WEIGHTS_FILE_NAME, map_location=map_location)\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create empty datasets\n",
      "create empty loaders\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_PATH = '../data/test'\n",
    "vanila_transform = transforms.Compose([\n",
    "                                transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor()\n",
    "                               ]) \n",
    "\n",
    "if not 'datasets' in locals():\n",
    "    print(\"create empty datasets\")\n",
    "    datasets = {}\n",
    "if not 'loaders' in locals():\n",
    "    print(\"create empty loaders\")\n",
    "\n",
    "loaders = {}\n",
    "datasets['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=vanila_transform)\n",
    "loaders['test'] = torch.utils.data.DataLoader(datasets['test'],\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.644501\n",
      "\n",
      "\n",
      "Test Accuracy: 50% (50/100)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition- Phase 1: generate the result csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model.load_state_dict(torch.load(TRAINED_MODEL_WEIGHTS_FILE_NAME))\n",
    "\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = ['A','B','C']\n",
    "\n",
    "def predict_class(img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                                transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor()\n",
    "                               ]) \n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0) \n",
    "\n",
    "    img = Variable(img)\n",
    "\n",
    "    img = img.to(device)\n",
    "        \n",
    "    prediction = model(img)  # Returns a Tensor of shape (batch, num class labels)\n",
    "    prediction = prediction.data.cpu().numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n",
    "    prediction = class_names[prediction]\n",
    "    return prediction \n",
    "\n",
    "\n",
    "predict_class('../data/competition/02186.jpg')\n",
    "\n",
    "#Get all test files\n",
    "\n",
    "test_results = []\n",
    "\n",
    "mango_files = np.array(glob(\"../data/competition/*\"))\n",
    "\n",
    "for idx, file in enumerate(mango_files):\n",
    "    _ , filename = os.path.split(file)\n",
    "    className = predict_class(file)\n",
    "    test_results.append([filename, className])\n",
    "    \n",
    "# test_results[:3]\n",
    "\n",
    "with open('results.csv', 'w') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for row in test_results:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
