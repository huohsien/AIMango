{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from PIL import ImageFile\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#-----\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "#-----\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "valid_loss_stable_count = 10\n",
    "num_training_epochs = 100\n",
    "exp_lr_scheduler_step_size = 7\n",
    "exp_lr_scheduler_gamma = 0.1\n",
    "sgd_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = '../C1-P1_Train Dev_fixed/train.csv'\n",
    "VALID_CSV_PATH = '../C1-P1_Train Dev_fixed/dev.csv'\n",
    "\n",
    "\n",
    "ORIGINAL_TRAIN_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "ORIGINAL_VALID_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Dev/' \n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train'\n",
    "VALID_DATA_PATH = '../data/valid'\n",
    "TEST_DATA_PATH = '../data/test'\n",
    "\n",
    "TRAINED_MODEL_WEIGHTS_FILE_NAME = 'model_v4_weights.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can choose not to use  data in \"../AIMango_sample/\" as test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data in \"../AIMango_sample/\" as test dataset for evaluation\n",
    "# the format of this dataset is different from the rest\n",
    "# the csv file looks like: D-Plant2_0610_3.jpg,等級B\n",
    "# the following code is to transform it into: D-Plant2_0610_3.jpg,B\n",
    "\n",
    "TEST_CSV_PATH = '../AIMango_sample/label.csv'\n",
    "ORIGINAL_TEST_DATA_PATH = '../AIMango_sample/sample_image/' \n",
    "\n",
    "new_data = []\n",
    "with open(TEST_CSV_PATH) as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        new_label = row[1][len(row[1])-1] # remove 等級 from the label 等級B and get the new lable B\n",
    "        new_data.append([row[0],new_label])\n",
    "\n",
    "\n",
    "# save the data from the original csv file that was transformed by the code above to a new csv file\n",
    "folder, filename = os.path.split(TEST_CSV_PATH)\n",
    "TEST_CSV_PATH = os.path.join(folder, 'label_new.csv')\n",
    "                                 \n",
    "with open(TEST_CSV_PATH, 'w') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for row in new_data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse csv file and make the folder structure for pytorch test dataset\n",
    "def prepare_file_structure_for_pytoch(csv_path, src_data_path, dst_data_path):\n",
    "    with open(csv_path) as csv_file:\n",
    "\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "                continue  #header\n",
    "\n",
    "            src_path = os.path.join(src_data_path, row[0])\n",
    "            dest_path = os.path.join(dst_data_path, row[1], row[0])\n",
    "            dest_folder_path = os.path.join(dst_data_path, row[1])\n",
    "            if not os.path.exists(dest_folder_path):\n",
    "                os.makedirs(dest_folder_path)\n",
    "            if not os.path.isfile(dest_path):\n",
    "                copyfile(src_path, dest_path)\n",
    "        \n",
    "            line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make file structure for training dataset\n",
    "prepare_file_structure_for_pytoch(TRAIN_CSV_PATH, ORIGINAL_TRAIN_DATA_PATH, TRAIN_DATA_PATH)\n",
    "    \n",
    "# make file structure for validation dataset\n",
    "prepare_file_structure_for_pytoch(VALID_CSV_PATH, ORIGINAL_VALID_DATA_PATH, VALID_DATA_PATH)\n",
    "\n",
    "# make file structure for test dataset\n",
    "prepare_file_structure_for_pytoch(TEST_CSV_PATH, ORIGINAL_TEST_DATA_PATH, TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "aug_transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(degrees=(-15, 15)),\n",
    "                                transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "\n",
    "vanila_transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ]) \n",
    "loaders = {}\n",
    "datasets = {}\n",
    "\n",
    "datasets['train'] = torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, transform=aug_transform)\n",
    "loaders['train'] = torch.utils.data.DataLoader(datasets['train'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "datasets['valid'] = torchvision.datasets.ImageFolder(VALID_DATA_PATH, transform=vanila_transform)\n",
    "loaders['valid'] = torch.utils.data.DataLoader(datasets['valid'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "datasets['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=vanila_transform)\n",
    "loaders['test'] = torch.utils.data.DataLoader(datasets['test'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /Users/huohsien/.torch/models/inception_v3_google-1a9a5a14.pth\n",
      "100%|██████████| 108857766/108857766 [00:09<00:00, 11831541.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#MODEL in evaluation\n",
    "\n",
    "model = models.inception_v3(pretrained=True)\n",
    "\n",
    "model.aux_logits=False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=sgd_momentum)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=exp_lr_scheduler_step_size, gamma=exp_lr_scheduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, scheduler, save_path):\n",
    "\n",
    "    best_acc = 0.0 \n",
    "\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "                \n",
    "        start = time.time()\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update accumulated training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        valid_corrects = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['valid']):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update accumulated validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "            # accumulate number of the accurate predictions\n",
    "            valid_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "        train_loss = train_loss/len(loaders['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
    "        \n",
    "        epoch_acc = valid_corrects.double() / len(loaders['valid'].dataset)\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.2f} \\ttime: {:.1f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            epoch_acc,\n",
    "            time.time() - start\n",
    "            ))\n",
    "        \n",
    "        if epoch_acc > best_acc:\n",
    "            print('Accuracy increased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                best_acc,\n",
    "                epoch_acc\n",
    "            ))\n",
    "\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, save_path)\n",
    "\n",
    "\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "model_transfer = train(num_training_epochs,\n",
    "                       loaders,\n",
    "                       model,\n",
    "                       optimizer,\n",
    "                       criterion,\n",
    "                       exp_lr_scheduler,\n",
    "                       TRAINED_MODEL_WEIGHTS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition- Phase 1: generate the result csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model.load_state_dict(torch.load(TRAINED_MODEL_WEIGHTS_FILE_NAME))\n",
    "\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = ['A','B','C']\n",
    "\n",
    "def predict_class_transfer(img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()\n",
    "                               ]) \n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0) \n",
    "\n",
    "    img = Variable(img)\n",
    "\n",
    "    img = img.to(device)\n",
    "        \n",
    "    prediction = model_transfer(img)  # Returns a Tensor of shape (batch, num class labels)\n",
    "    prediction = prediction.data.cpu().numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n",
    "    prediction = class_names[prediction]\n",
    "    return prediction \n",
    "\n",
    "\n",
    "predict_class_transfer('../data/competition/02186.jpg')\n",
    "\n",
    "#Get all test files\n",
    "\n",
    "test_results = []\n",
    "\n",
    "mango_files = np.array(glob(\"../data/competition/*\"))\n",
    "\n",
    "for idx, file in enumerate(mango_files):\n",
    "    _ , filename = os.path.split(file)\n",
    "    className = predict_class_transfer(file)\n",
    "    test_results.append([filename, className])\n",
    "    \n",
    "# test_results[:3]\n",
    "\n",
    "with open('results.csv', 'w') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for row in test_results:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following module can be run separately if trained weights are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The block below define the Model and should be exactly the same as the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.inception_v3(pretrained=True)\n",
    "\n",
    "model.aux_logits=False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model weights that got the best validation accuracy \n",
    "model.load_state_dict(torch.load(TRAINED_MODEL_WEIGHTS_FILE_NAME, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = '../data/test'\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()\n",
    "                               ]) \n",
    "\n",
    "if not 'datasets' in locals():\n",
    "    print(\"create empty datasets\")\n",
    "    datasets = {}\n",
    "if not 'loaders' in locals():\n",
    "    print(\"create empty loaders\")\n",
    "    loaders = {}\n",
    "datasets['test'] = datasets.ImageFolder(TEST_DATA_PATH, transform=transform)\n",
    "loaders['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_transfer, model_transfer, criterion_transfer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
