{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#### batch_size = 32\n",
    "#### num_training_epochs = 200\n",
    "#### lr = 0.001\n",
    "#### valid_loss_stable_count = 7\n",
    "\n",
    "#### Make learning rate lr one tenth if the number of epochs in which validation loss doesn't decrease exceeds the paramter of valid_loss_stable_count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_CSV_PATH = '../C1-P1_Train Dev_fixed/train.csv'\n",
    "TRAIN_CSV_PATH = '../C1-P1_Train Dev_fixed/train_split.csv'\n",
    "VALID_CSV_PATH = '../C1-P1_Train Dev_fixed/dev.csv'\n",
    "\n",
    "ORIGINAL_TRAIN_TEST_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "ORIGINAL_VALID_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Dev/' \n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train'\n",
    "VALID_DATA_PATH = '../data/valid'\n",
    "TEST_DATA_PATH = '../data/test'\n",
    "\n",
    "MODEL_WEIGHTS_FILE = '../model_trained_weights/model_weights_v3_1__run_3.pt'\n",
    "\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "valid_loss_stable_count = 7\n",
    "\n",
    "lr_decay_factor = 0.1\n",
    "lr_lower_bound = 1e-4\n",
    "\n",
    "num_training_epochs = 200\n",
    "num_worker = 4\n",
    "sgd_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(degrees=(-15, 15)),\n",
    "                                transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "transform_plain = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                           ]) \n",
    "\n",
    "loaders_transfer = {}\n",
    "data_transfer = {}\n",
    "\n",
    "data_transfer['train'] = torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, transform=transform)\n",
    "loaders_transfer['train'] = torch.utils.data.DataLoader(data_transfer['train'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_worker)\n",
    "\n",
    "data_transfer['valid'] = torchvision.datasets.ImageFolder(VALID_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['valid'] = torch.utils.data.DataLoader(data_transfer['valid'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_worker)\n",
    "data_transfer['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model_transfer.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 3)).to(device)\n",
    "## uncomment the following line for continuing trainging only\n",
    "# model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.SGD(model_transfer.fc.parameters(), lr = lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_from_optim(optimizer):\n",
    "    for param_group in optimizer_transfer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def set_lr_to_optim(optimizer, lr):\n",
    "    for param_group in optimizer_transfer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.8594 \tValidation Loss: 0.8075 \tValidation Accuracy: 0.589 \ttime: 213.2 \tlr=0.01\n",
      "Validation loss decreased from inf to 0.807500. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 0.7389 \tValidation Loss: 0.8630 \tValidation Accuracy: 0.585 \ttime: 208.7 \tlr=0.01\n",
      "Epoch: 3 \tTraining Loss: 0.7190 \tValidation Loss: 0.7267 \tValidation Accuracy: 0.639 \ttime: 209.2 \tlr=0.01\n",
      "Validation loss decreased from 0.807500 to 0.726673. Model was saved\n",
      "Epoch: 4 \tTraining Loss: 0.6704 \tValidation Loss: 0.6706 \tValidation Accuracy: 0.688 \ttime: 212.0 \tlr=0.01\n",
      "Validation loss decreased from 0.726673 to 0.670617. Model was saved\n",
      "Epoch: 5 \tTraining Loss: 0.6653 \tValidation Loss: 0.6335 \tValidation Accuracy: 0.719 \ttime: 212.5 \tlr=0.01\n",
      "Validation loss decreased from 0.670617 to 0.633460. Model was saved\n",
      "Epoch: 6 \tTraining Loss: 0.6497 \tValidation Loss: 0.6355 \tValidation Accuracy: 0.726 \ttime: 212.4 \tlr=0.01\n",
      "Epoch: 7 \tTraining Loss: 0.6489 \tValidation Loss: 0.6484 \tValidation Accuracy: 0.729 \ttime: 210.8 \tlr=0.01\n",
      "Epoch: 8 \tTraining Loss: 0.6619 \tValidation Loss: 0.6283 \tValidation Accuracy: 0.724 \ttime: 212.5 \tlr=0.01\n",
      "Validation loss decreased from 0.633460 to 0.628333. Model was saved\n",
      "Epoch: 9 \tTraining Loss: 0.6395 \tValidation Loss: 0.6168 \tValidation Accuracy: 0.734 \ttime: 211.4 \tlr=0.01\n",
      "Validation loss decreased from 0.628333 to 0.616837. Model was saved\n",
      "Epoch: 10 \tTraining Loss: 0.6485 \tValidation Loss: 0.6124 \tValidation Accuracy: 0.735 \ttime: 211.9 \tlr=0.01\n",
      "Validation loss decreased from 0.616837 to 0.612449. Model was saved\n",
      "Epoch: 11 \tTraining Loss: 0.6273 \tValidation Loss: 0.6766 \tValidation Accuracy: 0.693 \ttime: 212.0 \tlr=0.01\n",
      "Epoch: 12 \tTraining Loss: 0.6305 \tValidation Loss: 0.6497 \tValidation Accuracy: 0.698 \ttime: 211.9 \tlr=0.01\n",
      "Epoch: 13 \tTraining Loss: 0.6156 \tValidation Loss: 0.6573 \tValidation Accuracy: 0.708 \ttime: 208.7 \tlr=0.01\n",
      "Epoch: 14 \tTraining Loss: 0.6026 \tValidation Loss: 0.6101 \tValidation Accuracy: 0.735 \ttime: 211.9 \tlr=0.01\n",
      "Validation loss decreased from 0.612449 to 0.610089. Model was saved\n",
      "Epoch: 15 \tTraining Loss: 0.6030 \tValidation Loss: 0.6820 \tValidation Accuracy: 0.699 \ttime: 211.1 \tlr=0.01\n",
      "Epoch: 16 \tTraining Loss: 0.6153 \tValidation Loss: 0.6864 \tValidation Accuracy: 0.701 \ttime: 210.0 \tlr=0.01\n",
      "Epoch: 17 \tTraining Loss: 0.6111 \tValidation Loss: 0.5976 \tValidation Accuracy: 0.733 \ttime: 212.2 \tlr=0.01\n",
      "Validation loss decreased from 0.610089 to 0.597590. Model was saved\n",
      "Epoch: 18 \tTraining Loss: 0.6207 \tValidation Loss: 0.6030 \tValidation Accuracy: 0.748 \ttime: 214.3 \tlr=0.01\n",
      "Epoch: 19 \tTraining Loss: 0.6037 \tValidation Loss: 0.5911 \tValidation Accuracy: 0.743 \ttime: 212.3 \tlr=0.01\n",
      "Validation loss decreased from 0.597590 to 0.591149. Model was saved\n",
      "Epoch: 20 \tTraining Loss: 0.6211 \tValidation Loss: 0.6009 \tValidation Accuracy: 0.741 \ttime: 213.0 \tlr=0.01\n",
      "Epoch: 21 \tTraining Loss: 0.5818 \tValidation Loss: 0.6200 \tValidation Accuracy: 0.739 \ttime: 212.6 \tlr=0.01\n",
      "Epoch: 22 \tTraining Loss: 0.5881 \tValidation Loss: 0.6239 \tValidation Accuracy: 0.726 \ttime: 212.6 \tlr=0.01\n",
      "Epoch: 23 \tTraining Loss: 0.6097 \tValidation Loss: 0.6119 \tValidation Accuracy: 0.735 \ttime: 211.6 \tlr=0.01\n",
      "Epoch: 24 \tTraining Loss: 0.5862 \tValidation Loss: 0.6419 \tValidation Accuracy: 0.736 \ttime: 213.4 \tlr=0.01\n",
      "Epoch: 25 \tTraining Loss: 0.5758 \tValidation Loss: 0.5951 \tValidation Accuracy: 0.734 \ttime: 212.3 \tlr=0.01\n",
      "Epoch: 26 \tTraining Loss: 0.5796 \tValidation Loss: 0.5892 \tValidation Accuracy: 0.735 \ttime: 211.7 \tlr=0.01\n",
      "Validation loss decreased from 0.591149 to 0.589234. Model was saved\n",
      "Epoch: 27 \tTraining Loss: 0.5689 \tValidation Loss: 0.6053 \tValidation Accuracy: 0.743 \ttime: 209.9 \tlr=0.01\n",
      "Epoch: 28 \tTraining Loss: 0.5734 \tValidation Loss: 0.6791 \tValidation Accuracy: 0.680 \ttime: 209.6 \tlr=0.01\n",
      "Epoch: 29 \tTraining Loss: 0.6029 \tValidation Loss: 0.5853 \tValidation Accuracy: 0.751 \ttime: 211.6 \tlr=0.01\n",
      "Validation loss decreased from 0.589234 to 0.585250. Model was saved\n",
      "Epoch: 30 \tTraining Loss: 0.5791 \tValidation Loss: 0.5984 \tValidation Accuracy: 0.733 \ttime: 212.9 \tlr=0.01\n",
      "Epoch: 31 \tTraining Loss: 0.5660 \tValidation Loss: 0.5854 \tValidation Accuracy: 0.735 \ttime: 211.6 \tlr=0.01\n",
      "Epoch: 32 \tTraining Loss: 0.5788 \tValidation Loss: 0.6885 \tValidation Accuracy: 0.679 \ttime: 209.0 \tlr=0.01\n",
      "Epoch: 33 \tTraining Loss: 0.5688 \tValidation Loss: 0.6087 \tValidation Accuracy: 0.735 \ttime: 211.3 \tlr=0.01\n",
      "Epoch: 34 \tTraining Loss: 0.5548 \tValidation Loss: 0.5778 \tValidation Accuracy: 0.746 \ttime: 211.1 \tlr=0.01\n",
      "Validation loss decreased from 0.585250 to 0.577766. Model was saved\n",
      "Epoch: 35 \tTraining Loss: 0.5704 \tValidation Loss: 0.5755 \tValidation Accuracy: 0.756 \ttime: 213.1 \tlr=0.01\n",
      "Validation loss decreased from 0.577766 to 0.575538. Model was saved\n",
      "Epoch: 36 \tTraining Loss: 0.5556 \tValidation Loss: 0.5862 \tValidation Accuracy: 0.746 \ttime: 210.3 \tlr=0.01\n",
      "Epoch: 37 \tTraining Loss: 0.5585 \tValidation Loss: 0.6412 \tValidation Accuracy: 0.716 \ttime: 209.3 \tlr=0.01\n",
      "Epoch: 38 \tTraining Loss: 0.5571 \tValidation Loss: 0.6194 \tValidation Accuracy: 0.734 \ttime: 209.1 \tlr=0.01\n",
      "Epoch: 39 \tTraining Loss: 0.5548 \tValidation Loss: 0.6495 \tValidation Accuracy: 0.728 \ttime: 209.1 \tlr=0.01\n",
      "Epoch: 40 \tTraining Loss: 0.5664 \tValidation Loss: 0.5871 \tValidation Accuracy: 0.735 \ttime: 209.3 \tlr=0.01\n",
      "Epoch: 41 \tTraining Loss: 0.5551 \tValidation Loss: 0.5745 \tValidation Accuracy: 0.756 \ttime: 208.8 \tlr=0.01\n",
      "Validation loss decreased from 0.575538 to 0.574473. Model was saved\n",
      "Epoch: 42 \tTraining Loss: 0.5459 \tValidation Loss: 0.6201 \tValidation Accuracy: 0.734 \ttime: 213.3 \tlr=0.01\n",
      "Epoch: 43 \tTraining Loss: 0.5301 \tValidation Loss: 0.6299 \tValidation Accuracy: 0.720 \ttime: 212.7 \tlr=0.01\n",
      "Epoch: 44 \tTraining Loss: 0.5416 \tValidation Loss: 0.5878 \tValidation Accuracy: 0.734 \ttime: 211.7 \tlr=0.01\n",
      "Epoch: 45 \tTraining Loss: 0.5562 \tValidation Loss: 0.5931 \tValidation Accuracy: 0.731 \ttime: 212.0 \tlr=0.01\n",
      "Epoch: 46 \tTraining Loss: 0.5249 \tValidation Loss: 0.6092 \tValidation Accuracy: 0.749 \ttime: 211.8 \tlr=0.01\n",
      "Epoch: 47 \tTraining Loss: 0.5593 \tValidation Loss: 0.6135 \tValidation Accuracy: 0.726 \ttime: 211.5 \tlr=0.01\n",
      "Epoch: 48 \tTraining Loss: 0.5332 \tValidation Loss: 0.6116 \tValidation Accuracy: 0.730 \ttime: 213.7 \tlr=0.01\n",
      "Epoch: 49 \tTraining Loss: 0.5509 \tValidation Loss: 0.5896 \tValidation Accuracy: 0.748 \ttime: 212.2 \tlr=0.01\n",
      "Epoch: 50 \tTraining Loss: 0.5420 \tValidation Loss: 0.6686 \tValidation Accuracy: 0.704 \ttime: 212.9 \tlr=0.01\n",
      "Epoch: 51 \tTraining Loss: 0.5494 \tValidation Loss: 0.6080 \tValidation Accuracy: 0.731 \ttime: 210.3 \tlr=0.01\n",
      "Epoch: 52 \tTraining Loss: 0.4976 \tValidation Loss: 0.5877 \tValidation Accuracy: 0.749 \ttime: 212.6 \tlr=0.001\n",
      "Epoch: 53 \tTraining Loss: 0.5023 \tValidation Loss: 0.5835 \tValidation Accuracy: 0.749 \ttime: 213.0 \tlr=0.001\n",
      "Epoch: 54 \tTraining Loss: 0.4939 \tValidation Loss: 0.5858 \tValidation Accuracy: 0.751 \ttime: 212.6 \tlr=0.001\n",
      "Epoch: 55 \tTraining Loss: 0.4922 \tValidation Loss: 0.5875 \tValidation Accuracy: 0.743 \ttime: 214.0 \tlr=0.001\n",
      "Epoch: 56 \tTraining Loss: 0.4951 \tValidation Loss: 0.5901 \tValidation Accuracy: 0.746 \ttime: 211.5 \tlr=0.001\n",
      "Epoch: 57 \tTraining Loss: 0.4906 \tValidation Loss: 0.5854 \tValidation Accuracy: 0.739 \ttime: 210.9 \tlr=0.001\n",
      "Epoch: 58 \tTraining Loss: 0.4845 \tValidation Loss: 0.5859 \tValidation Accuracy: 0.745 \ttime: 212.7 \tlr=0.001\n",
      "Epoch: 59 \tTraining Loss: 0.4865 \tValidation Loss: 0.5861 \tValidation Accuracy: 0.745 \ttime: 211.8 \tlr=0.001\n",
      "Epoch: 60 \tTraining Loss: 0.4885 \tValidation Loss: 0.5856 \tValidation Accuracy: 0.748 \ttime: 213.0 \tlr=0.001\n",
      "Epoch: 61 \tTraining Loss: 0.4920 \tValidation Loss: 0.5795 \tValidation Accuracy: 0.749 \ttime: 211.9 \tlr=0.001\n",
      "Epoch: 62 \tTraining Loss: 0.4754 \tValidation Loss: 0.5856 \tValidation Accuracy: 0.751 \ttime: 211.7 \tlr=0.0001\n",
      "Epoch: 63 \tTraining Loss: 0.4783 \tValidation Loss: 0.5829 \tValidation Accuracy: 0.751 \ttime: 213.0 \tlr=0.0001\n",
      "Epoch: 64 \tTraining Loss: 0.4744 \tValidation Loss: 0.5843 \tValidation Accuracy: 0.750 \ttime: 211.6 \tlr=0.0001\n",
      "Epoch: 65 \tTraining Loss: 0.4735 \tValidation Loss: 0.5871 \tValidation Accuracy: 0.750 \ttime: 211.8 \tlr=0.0001\n",
      "Epoch: 66 \tTraining Loss: 0.4790 \tValidation Loss: 0.5819 \tValidation Accuracy: 0.755 \ttime: 212.8 \tlr=0.0001\n",
      "Epoch: 67 \tTraining Loss: 0.4734 \tValidation Loss: 0.5880 \tValidation Accuracy: 0.746 \ttime: 211.4 \tlr=0.0001\n",
      "Epoch: 68 \tTraining Loss: 0.4779 \tValidation Loss: 0.5888 \tValidation Accuracy: 0.746 \ttime: 212.4 \tlr=0.0001\n",
      "Epoch: 69 \tTraining Loss: 0.4776 \tValidation Loss: 0.5856 \tValidation Accuracy: 0.750 \ttime: 213.0 \tlr=0.0001\n",
      "Epoch: 70 \tTraining Loss: 0.4798 \tValidation Loss: 0.5887 \tValidation Accuracy: 0.740 \ttime: 212.1 \tlr=0.0001\n",
      "Epoch: 71 \tTraining Loss: 0.4905 \tValidation Loss: 0.5891 \tValidation Accuracy: 0.740 \ttime: 212.4 \tlr=0.0001\n",
      "Epoch: 72 \tTraining Loss: 0.4702 \tValidation Loss: 0.5833 \tValidation Accuracy: 0.751 \ttime: 212.4 \tlr=0.0001\n",
      "Epoch: 73 \tTraining Loss: 0.4830 \tValidation Loss: 0.5881 \tValidation Accuracy: 0.746 \ttime: 213.4 \tlr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/huohsien/.anaconda3/envs/dl/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a03a04ba7bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m                        \u001b[0mmodel_transfer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_transfer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                        \u001b[0mcriterion_transfer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                        MODEL_WEIGHTS_FILE)\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total training time: {:.2f} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a03a04ba7bed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, loaders, model, optimizer, criterion, save_path)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, save_path):\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    # Valid Loss Stable counter\n",
    "    valid_loss_stable_counter = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        lr = get_lr_from_optim(optimizer)\n",
    "        \n",
    "        # train the model\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # validate the model\n",
    "\n",
    "        model.eval()\n",
    "        valid_corrects = 0\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['valid']):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "            valid_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "        train_loss = train_loss/len(loaders_transfer['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders_transfer['valid'].dataset)\n",
    "        \n",
    "        epoch_acc = valid_corrects.double() / len(loaders_transfer['valid'].dataset)\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.3f} \\ttime: {:.1f} \\tlr={}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            epoch_acc,\n",
    "            time.time() - start,\n",
    "            lr\n",
    "            ))\n",
    "        \n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, save_path)\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "            valid_loss_stable_counter = 0\n",
    "        else:\n",
    "            valid_loss_stable_counter += 1\n",
    "            if valid_loss_stable_counter >= valid_loss_stable_count:\n",
    "                valid_loss_stable_counter = 0\n",
    "                lr = get_lr_from_optim(optimizer)\n",
    "                lr = lr * lr_decay_factor\n",
    "                if lr <= lr_lower_bound:\n",
    "                    lr = lr_lower_bound\n",
    "                    return model\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "train_start = time.time()\n",
    "model_transfer = train(num_training_epochs,\n",
    "                       loaders_transfer,\n",
    "                       model_transfer,optimizer_transfer,\n",
    "                       criterion_transfer,\n",
    "                       MODEL_WEIGHTS_FILE)\n",
    "print(\"Total training time: {:.2f} seconds\".format(time.time() - train_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following module can be run separately if trained weights are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_WEIGHTS_FILE = 'model_weights_v3_1__run_2.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.models as models\n",
    "# import torch.nn as nn\n",
    "# import torchvision.datasets\n",
    "# import torchvision.transforms as transforms\n",
    "# from PIL import ImageFile\n",
    "\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "# model_transfer.fc = nn.Sequential(\n",
    "#                nn.Linear(2048, 128),\n",
    "#                nn.ReLU(inplace=True),\n",
    "#                nn.Linear(128, 3)).to(device)\n",
    "# model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_DATA_PATH = '../data/test'\n",
    "# transform_plain = transforms.Compose([\n",
    "#                             transforms.Resize(224),\n",
    "#                             transforms.CenterCrop(224),\n",
    "#                             transforms.ToTensor(),\n",
    "#                             transforms.Normalize(\n",
    "#                                     mean=(0.485, 0.456, 0.406),\n",
    "#                                     std =(0.229, 0.224, 0.225))\n",
    "#                            ]) \n",
    "\n",
    "# if not 'data_transfer' in locals():\n",
    "#     print(\"create empty data_transfer\")\n",
    "#     data_transfer = {}\n",
    "# if not 'loaders_transfer' in locals():\n",
    "#     print(\"create empty loaders_transfer\")\n",
    "#     loaders_transfer = {}\n",
    "# data_transfer['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "# loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "#                                           batch_size=1,\n",
    "#                                           shuffle=False,\n",
    "#                                           num_workers=4)\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# criterion_transfer = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.681681\n",
      "\n",
      "\n",
      "Test Accuracy: 70% (140/200)\n",
      "Total testing time: 29.59 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def test(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # accumulate test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "        # compare predictions to true label\n",
    "        if torch.cuda.is_available():\n",
    "            correct += torch.sum(preds == target.data)\n",
    "        else:\n",
    "            correct += np.sum(np.squeeze(preds.eq(target.data.view_as(preds))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "        \n",
    "    test_loss = test_loss/len(loaders_transfer['test'].dataset)      \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function   \n",
    "test_start = time.time()\n",
    "test(loaders_transfer, model_transfer, criterion_transfer)\n",
    "print(\"Total testing time: {:.2f} seconds\".format(time.time() - test_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
