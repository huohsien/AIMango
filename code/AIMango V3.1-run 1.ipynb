{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#### batch_size = 128\n",
    "#### start_epoch = 0\n",
    "#### lr = 0.001\n",
    "#### valid_loss_stable_count = 15\n",
    "\n",
    "#### Make learning rate lr one tenth if the number of epochs in which validation loss doesn't decrease exceeds the paramter of valid_loss_stable_count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_CSV_PATH = '../C1-P1_Train Dev_fixed/train.csv'\n",
    "TRAIN_CSV_PATH = '../C1-P1_Train Dev_fixed/train_split.csv'\n",
    "VALID_CSV_PATH = '../C1-P1_Train Dev_fixed/dev.csv'\n",
    "\n",
    "ORIGINAL_TRAIN_TEST_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Train/' \n",
    "ORIGINAL_VALID_DATA_PATH = '../C1-P1_Train Dev_fixed/C1-P1_Dev/' \n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train'\n",
    "VALID_DATA_PATH = '../data/valid'\n",
    "TEST_DATA_PATH = '../data/test'\n",
    "\n",
    "MODEL_WEIGHTS_FILE = 'model_weights_v3_1__run_1.pt'\n",
    "\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "valid_loss_stable_count = 15\n",
    "num_training_epochs = 150\n",
    "num_worker = 4\n",
    "sgd_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(degrees=(-15, 15)),\n",
    "                                transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "transform_plain = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                           ]) \n",
    "\n",
    "loaders_transfer = {}\n",
    "data_transfer = {}\n",
    "\n",
    "data_transfer['train'] = torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, transform=transform)\n",
    "loaders_transfer['train'] = torch.utils.data.DataLoader(data_transfer['train'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_worker)\n",
    "\n",
    "data_transfer['valid'] = torchvision.datasets.ImageFolder(VALID_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['valid'] = torch.utils.data.DataLoader(data_transfer['valid'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_worker)\n",
    "data_transfer['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model_transfer.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 3)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = [optim.SGD(model_transfer.fc.parameters(), lr = lr, momentum = 0.9),\n",
    "                      optim.SGD(model_transfer.fc.parameters(), lr = lr * 0.1, momentum = 0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.077533 \tValidation Loss: 1.042720 \tValidation Accuracy: 0.5450 \ttime: 81.9\n",
      "Validation loss decreased from inf to 1.042720. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 1.013322 \tValidation Loss: 0.973690 \tValidation Accuracy: 0.6650 \ttime: 82.5\n",
      "Validation loss decreased from 1.042720 to 0.973690. Model was saved\n",
      "Epoch: 3 \tTraining Loss: 0.938187 \tValidation Loss: 0.898234 \tValidation Accuracy: 0.6763 \ttime: 81.4\n",
      "Validation loss decreased from 0.973690 to 0.898234. Model was saved\n",
      "Epoch: 4 \tTraining Loss: 0.862573 \tValidation Loss: 0.826509 \tValidation Accuracy: 0.6763 \ttime: 82.6\n",
      "Validation loss decreased from 0.898234 to 0.826509. Model was saved\n",
      "Epoch: 5 \tTraining Loss: 0.804107 \tValidation Loss: 0.775547 \tValidation Accuracy: 0.6813 \ttime: 81.6\n",
      "Validation loss decreased from 0.826509 to 0.775547. Model was saved\n",
      "Epoch: 6 \tTraining Loss: 0.763482 \tValidation Loss: 0.745710 \tValidation Accuracy: 0.6775 \ttime: 82.7\n",
      "Validation loss decreased from 0.775547 to 0.745710. Model was saved\n",
      "Epoch: 7 \tTraining Loss: 0.725855 \tValidation Loss: 0.715380 \tValidation Accuracy: 0.6925 \ttime: 81.7\n",
      "Validation loss decreased from 0.745710 to 0.715380. Model was saved\n",
      "Epoch: 8 \tTraining Loss: 0.703840 \tValidation Loss: 0.693271 \tValidation Accuracy: 0.7025 \ttime: 81.1\n",
      "Validation loss decreased from 0.715380 to 0.693271. Model was saved\n",
      "Epoch: 9 \tTraining Loss: 0.686369 \tValidation Loss: 0.677760 \tValidation Accuracy: 0.7150 \ttime: 82.2\n",
      "Validation loss decreased from 0.693271 to 0.677760. Model was saved\n",
      "Epoch: 10 \tTraining Loss: 0.677597 \tValidation Loss: 0.682022 \tValidation Accuracy: 0.7063 \ttime: 81.1\n",
      "Epoch: 11 \tTraining Loss: 0.671102 \tValidation Loss: 0.673527 \tValidation Accuracy: 0.7025 \ttime: 81.0\n",
      "Validation loss decreased from 0.677760 to 0.673527. Model was saved\n",
      "Epoch: 12 \tTraining Loss: 0.667257 \tValidation Loss: 0.670143 \tValidation Accuracy: 0.7025 \ttime: 81.8\n",
      "Validation loss decreased from 0.673527 to 0.670143. Model was saved\n",
      "Epoch: 13 \tTraining Loss: 0.661426 \tValidation Loss: 0.668366 \tValidation Accuracy: 0.7050 \ttime: 82.8\n",
      "Validation loss decreased from 0.670143 to 0.668366. Model was saved\n",
      "Epoch: 14 \tTraining Loss: 0.660805 \tValidation Loss: 0.667986 \tValidation Accuracy: 0.7013 \ttime: 83.5\n",
      "Validation loss decreased from 0.668366 to 0.667986. Model was saved\n",
      "Epoch: 15 \tTraining Loss: 0.661454 \tValidation Loss: 0.668583 \tValidation Accuracy: 0.7087 \ttime: 82.0\n",
      "Epoch: 16 \tTraining Loss: 0.668065 \tValidation Loss: 0.666833 \tValidation Accuracy: 0.7113 \ttime: 80.4\n",
      "Validation loss decreased from 0.667986 to 0.666833. Model was saved\n",
      "Epoch: 17 \tTraining Loss: 0.664097 \tValidation Loss: 0.666111 \tValidation Accuracy: 0.7087 \ttime: 81.1\n",
      "Validation loss decreased from 0.666833 to 0.666111. Model was saved\n",
      "Epoch: 18 \tTraining Loss: 0.663522 \tValidation Loss: 0.666618 \tValidation Accuracy: 0.7113 \ttime: 81.6\n",
      "Epoch: 19 \tTraining Loss: 0.662766 \tValidation Loss: 0.665205 \tValidation Accuracy: 0.7050 \ttime: 82.1\n",
      "Validation loss decreased from 0.666111 to 0.665205. Model was saved\n",
      "Epoch: 20 \tTraining Loss: 0.660527 \tValidation Loss: 0.668515 \tValidation Accuracy: 0.7013 \ttime: 83.0\n",
      "Epoch: 21 \tTraining Loss: 0.664769 \tValidation Loss: 0.662686 \tValidation Accuracy: 0.7075 \ttime: 81.8\n",
      "Validation loss decreased from 0.665205 to 0.662686. Model was saved\n",
      "Epoch: 22 \tTraining Loss: 0.657939 \tValidation Loss: 0.662137 \tValidation Accuracy: 0.7113 \ttime: 80.7\n",
      "Validation loss decreased from 0.662686 to 0.662137. Model was saved\n",
      "Epoch: 23 \tTraining Loss: 0.656099 \tValidation Loss: 0.663409 \tValidation Accuracy: 0.7163 \ttime: 82.4\n",
      "Epoch: 24 \tTraining Loss: 0.658257 \tValidation Loss: 0.660243 \tValidation Accuracy: 0.7188 \ttime: 80.8\n",
      "Validation loss decreased from 0.662137 to 0.660243. Model was saved\n",
      "Epoch: 25 \tTraining Loss: 0.657299 \tValidation Loss: 0.660418 \tValidation Accuracy: 0.7188 \ttime: 81.8\n",
      "Epoch: 26 \tTraining Loss: 0.661101 \tValidation Loss: 0.660795 \tValidation Accuracy: 0.7113 \ttime: 80.8\n",
      "Epoch: 27 \tTraining Loss: 0.659654 \tValidation Loss: 0.662442 \tValidation Accuracy: 0.7050 \ttime: 82.0\n",
      "Epoch: 28 \tTraining Loss: 0.657166 \tValidation Loss: 0.664030 \tValidation Accuracy: 0.7000 \ttime: 82.2\n",
      "Epoch: 29 \tTraining Loss: 0.654428 \tValidation Loss: 0.659939 \tValidation Accuracy: 0.7137 \ttime: 82.0\n",
      "Validation loss decreased from 0.660243 to 0.659939. Model was saved\n",
      "Epoch: 30 \tTraining Loss: 0.661407 \tValidation Loss: 0.660230 \tValidation Accuracy: 0.7125 \ttime: 81.7\n",
      "Epoch: 31 \tTraining Loss: 0.659752 \tValidation Loss: 0.658373 \tValidation Accuracy: 0.7150 \ttime: 80.5\n",
      "Validation loss decreased from 0.659939 to 0.658373. Model was saved\n",
      "Epoch: 32 \tTraining Loss: 0.655038 \tValidation Loss: 0.661115 \tValidation Accuracy: 0.7075 \ttime: 81.6\n",
      "Epoch: 33 \tTraining Loss: 0.660415 \tValidation Loss: 0.656555 \tValidation Accuracy: 0.7175 \ttime: 82.2\n",
      "Validation loss decreased from 0.658373 to 0.656555. Model was saved\n",
      "Epoch: 34 \tTraining Loss: 0.648367 \tValidation Loss: 0.656164 \tValidation Accuracy: 0.7113 \ttime: 80.3\n",
      "Validation loss decreased from 0.656555 to 0.656164. Model was saved\n",
      "Epoch: 35 \tTraining Loss: 0.653792 \tValidation Loss: 0.655904 \tValidation Accuracy: 0.7188 \ttime: 79.9\n",
      "Validation loss decreased from 0.656164 to 0.655904. Model was saved\n",
      "Epoch: 36 \tTraining Loss: 0.660049 \tValidation Loss: 0.657879 \tValidation Accuracy: 0.7075 \ttime: 82.6\n",
      "Epoch: 37 \tTraining Loss: 0.647561 \tValidation Loss: 0.658728 \tValidation Accuracy: 0.7113 \ttime: 82.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train(n_epochs, loaders, model, optimizers, criterion, save_path):\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    # Valid Loss Stable counter\n",
    "    valid_loss_stable_counter = 0\n",
    "    #optimizer index\n",
    "    optim_idx = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        optimizer = optimizers[optim_idx]\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # train the model\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # validate the model\n",
    "\n",
    "        model.eval()\n",
    "        valid_corrects = 0\n",
    "        for batch_idx, (data, target) in enumerate(loaders_transfer['valid']):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "            valid_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "        train_loss = train_loss/len(loaders_transfer['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders_transfer['valid'].dataset)\n",
    "        \n",
    "        epoch_acc = valid_corrects.double() / len(loaders_transfer['valid'].dataset)\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.4f} \\ttime: {:.1f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            epoch_acc,\n",
    "            time.time() - start\n",
    "            ))\n",
    "        \n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, save_path)\n",
    "\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "            valid_loss_stable_counter = 0\n",
    "        else:\n",
    "            valid_loss_stable_counter += 1\n",
    "            optim_idx = 1\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "train_start = time.time()\n",
    "model_transfer = train(num_training_epochs,\n",
    "                       loaders_transfer,\n",
    "                       model_transfer,optimizer_transfer,\n",
    "                       criterion_transfer,\n",
    "                       MODEL_WEIGHTS_FILE)\n",
    "print(\"Total training time: {:.2f} seconds\".format(time.time() - train_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following module can be run separately if trained weights are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS_FILE = 'model_weights_v3_1__run_1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_transfer = models.resnet152(pretrained=True).to(device)\n",
    "    \n",
    "model_transfer.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 3)).to(device)\n",
    "model_transfer.load_state_dict(torch.load(MODEL_WEIGHTS_FILE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = '../data/test'\n",
    "transform_plain = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(\n",
    "                                    mean=(0.485, 0.456, 0.406),\n",
    "                                    std =(0.229, 0.224, 0.225))\n",
    "                           ]) \n",
    "\n",
    "if not 'data_transfer' in locals():\n",
    "    print(\"create empty data_transfer\")\n",
    "    data_transfer = {}\n",
    "if not 'loaders_transfer' in locals():\n",
    "    print(\"create empty loaders_transfer\")\n",
    "    loaders_transfer = {}\n",
    "data_transfer['test'] = torchvision.datasets.ImageFolder(TEST_DATA_PATH, transform=transform_plain)\n",
    "loaders_transfer['test'] = torch.utils.data.DataLoader(data_transfer['test'],\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def test(loaders, model, criterion):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # accumulate test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        preds = output.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "        # compare predictions to true label\n",
    "        if torch.cuda.is_available():\n",
    "            correct += torch.sum(preds == target.data)\n",
    "        else:\n",
    "            correct += np.sum(np.squeeze(preds.eq(target.data.view_as(preds))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "        \n",
    "    test_loss = test_loss/len(loaders_transfer['test'].dataset)      \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function   \n",
    "test_start = time.time()\n",
    "test(loaders_transfer, model_transfer, criterion_transfer)\n",
    "print(\"Total testing time: {:.2f} seconds\".format(time.time() - test_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
